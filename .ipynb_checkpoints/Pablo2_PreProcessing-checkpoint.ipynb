{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essentials\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('train.xlsx') # Load the `train` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "def special_char(text):\n",
    "    \"\"\"Retrieve the special characters\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\W', ' ', text)\n",
    "\n",
    "def filter_single(text):\n",
    "    \"\"\"remove all single characters\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "def filter_singleStart(text):\n",
    "    \"\"\"Remove single characters from the start\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "def filter_multiplespace(text):\n",
    "    \"\"\"Substituting multiple spaces with single space\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "def stop_words():\n",
    "    \"\"\"Retrieve the stop words for vectorization -Feel free to modify this function\n",
    "    \"\"\"\n",
    "    return get_stop_words('es') + get_stop_words('ca') + get_stop_words('en')\n",
    "\n",
    "def filter_mentions(text):\n",
    "    \"\"\"Utility function to remove the mentions of a tweet\n",
    "    \"\"\"\n",
    "    return re.sub(\"@\\S+\", \"\", text)\n",
    "\n",
    "def filter_hashtags(text):\n",
    "    \"\"\"Utility function to remove the hashtags of a tweet\n",
    "    \"\"\"\n",
    "    return re.sub(\"#\\S+\", \"\", text)\n",
    "\n",
    "def filter_symb_hashtag(text):\n",
    "    \"\"\"Utility function to remove the hashtags symbol of a tweet\n",
    "    \"\"\"\n",
    "    return re.sub(\"#\", \"\", text)\n",
    "\n",
    "def filter_symb1(text):\n",
    "    \"\"\"Utility function to remove special characters of a tweet\n",
    "    \"\"\"\n",
    "    return re.sub(r'[(:+*?Â¿!Â¡.,;-`\"\")]', \"\", text)\n",
    "\n",
    "#def translate_text(text):\n",
    "#    \"\"\"Utility function to translate the text of a tweet\n",
    "#    \"\"\"\n",
    "#    return translator.translate(text, dest='es').text\n",
    "\n",
    "def lower_text(text):\n",
    "    \"\"\"Utility function to lower text\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def eliminate_numbers(text):\n",
    "    \"\"\"Utility function to lower text\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\w*\\d\\w*', '', text).strip()\n",
    "\n",
    "def eliminate_emojiis(text):\n",
    "    \"\"\"Utility to eliminate emojiis from the text\n",
    "    \"\"\"\n",
    "    em_pat=re.compile(\"[\"\n",
    "       u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return  em_pat.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>username</th>\n",
       "      <th>party</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>language</th>\n",
       "      <th>lang_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>martarovira</td>\n",
       "      <td>erc</td>\n",
       "      <td>Ãšltim acte de campanya! AquÃ­ tossudament alÃ§at...</td>\n",
       "      <td>2017-12-19 20:12:01</td>\n",
       "      <td>785</td>\n",
       "      <td>2295</td>\n",
       "      <td>ca</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>xavierdomenechs</td>\n",
       "      <td>comuns</td>\n",
       "      <td>#Badalona necessita uns pressupostos que posin...</td>\n",
       "      <td>2018-04-27 10:04:19</td>\n",
       "      <td>55</td>\n",
       "      <td>93</td>\n",
       "      <td>ca</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>albert_rivera</td>\n",
       "      <td>cs</td>\n",
       "      <td>Encuentro VillacÃ­s-Valls para lanzar una estra...</td>\n",
       "      <td>2018-11-17 20:34:58</td>\n",
       "      <td>357</td>\n",
       "      <td>622</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>jaumecollboni</td>\n",
       "      <td>psc</td>\n",
       "      <td>â€œLa palabra es como una bala, no tiene retorno...</td>\n",
       "      <td>2018-10-22 18:10:01</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>albiol_xg</td>\n",
       "      <td>ppc</td>\n",
       "      <td>ðŸ“» Esta noche, a partir de las 22:10h, me entre...</td>\n",
       "      <td>2018-08-16 10:30:27</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id         username   party  \\\n",
       "0   0      martarovira     erc   \n",
       "1   1  xavierdomenechs  comuns   \n",
       "2   2    albert_rivera      cs   \n",
       "3   3    jaumecollboni     psc   \n",
       "4   4        albiol_xg     ppc   \n",
       "\n",
       "                                                text          created_at  \\\n",
       "0  Ãšltim acte de campanya! AquÃ­ tossudament alÃ§at... 2017-12-19 20:12:01   \n",
       "1  #Badalona necessita uns pressupostos que posin... 2018-04-27 10:04:19   \n",
       "2  Encuentro VillacÃ­s-Valls para lanzar una estra... 2018-11-17 20:34:58   \n",
       "3  â€œLa palabra es como una bala, no tiene retorno... 2018-10-22 18:10:01   \n",
       "4  ðŸ“» Esta noche, a partir de las 22:10h, me entre... 2018-08-16 10:30:27   \n",
       "\n",
       "   retweet_count  favorite_count language  lang_val  \n",
       "0            785            2295       ca         1  \n",
       "1             55              93       ca         1  \n",
       "2            357             622       es         0  \n",
       "3              4               6       es         0  \n",
       "4             20              47       es         0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate text to spanish and define column (vector) with language (spa:0, cat:1)\n",
    "#!pip install langdetect\n",
    "from langdetect import detect \n",
    "train_df['language'] = [detect(row['text']) for index, row in train_df.iterrows()]\n",
    "\n",
    "# language column to 1-0 --> esta columna lang_val la ponemos al final del array (Xvec)\n",
    "train_df['lang_val'] = np.where(train_df.language == 'ca', 1, 0)\n",
    "train_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['traduc'] = np.nan\n",
    "for i in range(len(train_df)):\n",
    "    if train_df['lang_val'][i]==1:\n",
    "        translator = Translator()\n",
    "        a = translator.translate(text = train_df[\"text\"][i],dest=\"es\").text\n",
    "        train_df['traduc'][i]=a\n",
    "        print(a)\n",
    "        \n",
    "    else:\n",
    "        train_df['traduc'][i]=train_df['text'][i]\n",
    "        print('nothing')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall googletrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/BoseCorp/py-googletrans.git\n",
      "  Cloning https://github.com/BoseCorp/py-googletrans.git to /tmp/pip-req-build-2hd06x8v\n",
      "Requirement already satisfied, skipping upgrade: requests in /home/pablo/anaconda3/lib/python3.7/site-packages (from googletrans==2.3.0) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/pablo/anaconda3/lib/python3.7/site-packages (from requests->googletrans==2.3.0) (2018.11.29)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/pablo/anaconda3/lib/python3.7/site-packages (from requests->googletrans==2.3.0) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/pablo/anaconda3/lib/python3.7/site-packages (from requests->googletrans==2.3.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /home/pablo/anaconda3/lib/python3.7/site-packages (from requests->googletrans==2.3.0) (1.24.1)\n",
      "Building wheels for collected packages: googletrans\n",
      "  Running setup.py bdist_wheel for googletrans ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-vp52_vcs/wheels/6a/fc/9e/2d31d95d9e97da5166afd8225a6f3b6850dc2c6e84accefbfc\n",
      "Successfully built googletrans\n",
      "Installing collected packages: googletrans\n",
      "  Found existing installation: googletrans 2.3.0\n",
      "    Uninstalling googletrans-2.3.0:\n",
      "      Successfully uninstalled googletrans-2.3.0\n",
      "Successfully installed googletrans-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/BoseCorp/py-googletrans.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-cf92db4c2488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Hola com estem'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'es'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ca'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mformat_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacy_format_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/googletrans/utils.py\u001b[0m in \u001b[0;36mlegacy_format_json\u001b[0;34m(original)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "translator.translate('Hola com estem', dest='es', src='ca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install googletrans\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "import time\n",
    "import copy\n",
    "\n",
    "translatedList = []\n",
    "for index, row in train_df.iterrows():\n",
    "    if train_df['lang_val'][i]==1:\n",
    "        # REINITIALIZE THE API\n",
    "        translator = Translator()\n",
    "        newrow = copy.deepcopy(row)\n",
    "        try:\n",
    "            # translate the 'text' column\n",
    "            translated = translator.translate(row['text'], dest='es')\n",
    "            newrow['translated'] = translated.text\n",
    "            print(translated.text)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            time.sleep(5)\n",
    "            continue\n",
    "        translatedList.append(newrow)\n",
    "    else:\n",
    "        newrow = copy.deepcopy(row)\n",
    "        translatedList.append(newrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prova amb traductor posant un standby\n",
    "#!pip install googletrans\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "\n",
    "\n",
    "    \n",
    "for index, row in train_df.iterrows():\n",
    "    #row['text']=emoji_pattern.sub(r'', row['text'])\n",
    "    if row['lang_val'] == 1:\n",
    "        print(row['text'])\n",
    "        #traduct = Translator.translate(text=row['text'], dest='es').text\n",
    "        traduct = translator.translate(text=row['text'], dest='es')\n",
    "        train_df['traduc'] = traduct.text\n",
    "        #print(traduct.text)\n",
    "        \n",
    "        #print(row['traduccio'])\n",
    "    else:\n",
    "        train_df['traduc'] = row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['traduc'].head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing (1-3 fets)\n",
    "1) eliminate symbols :,?Â¿!';-\n",
    "2) eliminate single characters  \n",
    "2b) numbers (eliminate numbers and words with numbers)  \n",
    "3) all to lower  \n",
    "4) accents? (amb unidecode, pero passa Ã§ a c, Ã± a n, aixo introdueix errors?)  \n",
    "5) i is fessim una columna amb 0-1 indicant si hi ha emoji? (sigui com sigui, pero algu potser els fa servir bastant?)\n",
    "6) segurament, si hi ha hashtag del partit l'ha posat el mateix partit (publicitat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text_clean'] = [filter_symb1(filter_symb_hashtag(eliminate_emojiis(filter_single(eliminate_numbers(lower_text(row['text'])))))) for index, row in train_df.iterrows()]\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_df.text[12])\n",
    "print (train_df.text_clean[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Afegim una columna amb idioma\n",
    "tot i que alguns resultats ens han mostrat pitjors scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### coses per fer:\n",
    "- lemmatization: sino ho podem traduir (el googletranslator dona problemes) no el podem aplicar (no n'hi ha en catala)  \n",
    "- identificar hastags? ara per ara han quedat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "Ho fem fent servir el pipeline (aixo ens permet aplicar una funcio per buscar parametres optims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC\n",
    "### Resultats\n",
    "1. pipeline per trobar millors parametres: aobre text filtrats numbers, simbols especials i single letters (queden hashtags i emojis) \n",
    "score = 0.72 -->  Text Clean  \n",
    "{'clf__C': 1.0, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 1, 'clf__penalty': 'l2', 'vect__max_features': 6000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words':\n",
    "2. apliquem model sobre variacions del vector? per exemple treient hasthtags?  \n",
    "score = 0.73  --> Clean i eliminants Hasthtags\n",
    "{'clf__C': 1.0, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 10, 'clf__penalty': 'l2', 'vect__max_features': 6000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words'\n",
    "3. eliminem tb emojiis\n",
    "score= 0.73\n",
    "{'clf__C': 1.0, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 10, 'clf__penalty': 'l2', 'vect__max_features': 6000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text_clean'], train_df['party'].values, test_size=0.20, random_state=345 )\n",
    "param_grid = [\n",
    "    { \n",
    "          \"clf__penalty\": [\"l2\"],\n",
    "          \"clf__dual\":[False,True],\n",
    "          \"clf__C\":[0.1, 1.0],\n",
    "          \"clf__fit_intercept\":[True,False],\n",
    "          \"clf__intercept_scaling\":[0.1,1,10],\n",
    "          \"vect__min_df\":[1,2,3,4],\n",
    "          \"vect__ngram_range\": [(1, 1), (1, 2),(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000, 8000, 10000]\n",
    "    },\n",
    "    { \n",
    "          \"clf__penalty\": [\"l1\"],\n",
    "          \"clf__dual\":[False],\n",
    "          \"clf__C\":[0.1, 1.0],\n",
    "          \"clf__fit_intercept\":[True,False],\n",
    "          \"clf__intercept_scaling\":[0.1,1,10],\n",
    "          \"vect__min_df\":[1,2,3,4],\n",
    "          \"vect__ngram_range\": [(1, 1),(1, 2),(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000, 8000, 10000]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=6, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print (\"score = %3.2f\" %(grid.score(X_test,y_test)) )\n",
    "print (pd.DataFrame.from_dict(grid.cv_results_).loc[grid.best_index_ ])\n",
    "print (grid.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text_clean'], train_df['party'].values, test_size=0.20, random_state=345 )\n",
    "vectorizer= TfidfVectorizer(min_df=1, ngram_range=(1,2), max_features=8000, stop_words=stop_words())\n",
    "\n",
    "Xvec=vectorizer.fit_transform(X_train).toarray()\n",
    "Xvec_test=vectorizer.transform(X_test).toarray()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tot = vectorizer.fit_transform(train_df['text_clean']).toarray()\n",
    "y_tot = train_df['party'].values\n",
    "\n",
    "\n",
    "from sklearn import manifold\n",
    "n_components = 3\n",
    "perplexity = 100\n",
    "tsne = manifold.TSNE(n_components=n_components, init='random',random_state=0, perplexity=perplexity)\n",
    "X_tsne = tsne.fit_transform(X_tot)\n",
    "b,w = np.unique(y_tot.tolist(), return_inverse=True)\n",
    "\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "#With TSNE\n",
    "plt.figure()\n",
    "scatter = plt.scatter(X_tsne[:,0], X_tsne[:,1],c=w,cmap=cm.get_cmap('Accent'))\n",
    "plt.title(\"T-SNE \")\n",
    "plt.xlabel(\"1st Component\")\n",
    "plt.ylabel(\"2n Component\")\n",
    "\n",
    "#aÃ±adir leyenda (ni puta idea que fa aqui)\n",
    "labels = np.unique(w)\n",
    "labels2= b\n",
    "handles = [plt.Line2D([],[],marker=\"o\", ls=\"\", \n",
    "                      color=scatter.cmap(scatter.norm(yi))) for yi in labels]\n",
    "plt.legend(handles, labels2,loc='best')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "scatter = plt.scatter(X_tsne[:,0], X_tsne[:,1],c=w,cmap=cm.get_cmap('Accent'))\n",
    "plt.title(\"T-SNE \")\n",
    "plt.xlabel(\"1st Component\")\n",
    "plt.ylabel(\"2n Component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bokeh\n",
    "from bokeh.plotting import figure, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\n",
    "    \"#%02x%02x%02x\" % (int(r), int(g), 150) for r, g in zip(100*X_tsne[:,0], 2*X_tsne[:,1])\n",
    "]\n",
    "\n",
    "p=figure(plot_width=800, plot_height=400)\n",
    "p.scatter(X_tsne[:,0], X_tsne[:,1],color='inferno',fill_alpha=.2)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=LinearSVC(C=1, dual=False, fit_intercept=True, intercept_scaling=10,penalty='l2',)\n",
    "clf.fit(Xvec,y_train)\n",
    "clf.predict(Xvec_test)\n",
    "\n",
    "print (\"score = %3.2f\" %(clf.score(Xvec_test,y_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
