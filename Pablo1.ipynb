{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@martinpella/customers-tweets-classification-41cdca4e2de\n",
    "# https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "#https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a\n",
    "# https://datascienceplus.com/multi-class-text-classification-with-scikit-learn/\n",
    "\n",
    "# Import essentials\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('train.xlsx') # Load the `train` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_excel('test.xlsx') # Load the `test` file\n",
    "#test_df.sample(frac=0.1)[:10] # Show a sample of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "def special_char(text):\n",
    "    \"\"\"Retrieve the special characters\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\W', ' ', text)\n",
    "\n",
    "def filter_single(text):\n",
    "    \"\"\"remove all single characters\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "def filter_singleStart(text):\n",
    "    \"\"\"Remove single characters from the start\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\^[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "def filter_multiplespace(text):\n",
    "    \"\"\"Substituting multiple spaces with single space\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "def stop_words():\n",
    "    \"\"\"Retrieve the stop words for vectorization -Feel free to modify this function\n",
    "    \"\"\"\n",
    "    return get_stop_words('es') + get_stop_words('ca') + get_stop_words('en')\n",
    "\n",
    "def filter_mentions(text):\n",
    "    \"\"\"Utility function to remove the mentions of a tweet\n",
    "    \"\"\"\n",
    "    return re.sub(\"@\\S+\", \"\", text)\n",
    "\n",
    "def filter_hashtags(text):\n",
    "    \"\"\"Utility function to remove the hashtags of a tweet\n",
    "    \"\"\"\n",
    "    return re.sub(\"#\\S+\", \"\", text)\n",
    "\n",
    "def filter_symb_hashtag(text):\n",
    "    \"\"\"Utility function to remove the hashtags symbol of a tweet\n",
    "    \"\"\"\n",
    "    return re.sub(\"#\", \"\", text)\n",
    "\n",
    "#def translate_text(text):\n",
    "#    \"\"\"Utility function to translate the text of a tweet\n",
    "#    \"\"\"\n",
    "#    return translator.translate(text, dest='es').text\n",
    "\n",
    "def lower_text(text):\n",
    "    \"\"\"Utility function to lower text\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def eliminate_emojiis(text):\n",
    "    \"\"\"Utility to eliminate emojiis from the text\n",
    "    \"\"\"\n",
    "    em_pat=re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return  em_pat.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis --> quedan emojis\n",
    "train_df['text_noEm']=[eliminate_emojiis(row['text']) for index, row in train_df.iterrows()]\n",
    "\n",
    "# from no emojiis we apply the other filters (we consider emojiis are not useful here...why?)\n",
    "#train_df['text_noEm_noMentions'] = [filter_mentions(row['text_noEm']) for index, row in train_df.iterrows()]\n",
    "#train_df['text_noEm_noHashtags'] = [filter_hashtags(row['text_noEm']) for index, row in train_df.iterrows()]\n",
    "#train_df['text_noEm_noSymbHashtags'] = [filter_symb_hashtag(row['text_noEm']) for index, row in train_df.iterrows()]\n",
    "train_df['text_noSymbHashtags'] = [filter_symb_hashtag(row['text']) for index, row in train_df.iterrows()]\n",
    "# to spanish? with and without mentions and hashtags\n",
    "#train_df['text_noEM_sp'] = [translate_text(row['text_noEm']) for index, row in train_df.iterrows()]\n",
    "train_df['text_noSymbHashtags_more']=[special_char(row['text_noSymbHashtags']) for index, row in train_df.iterrows()]\n",
    "train_df['text_noSymbHashtags_more']=[filter_single(row['text_noSymbHashtags']) for index, row in train_df.iterrows()]\n",
    "train_df['text_noSymbHashtags_more']=[lower_text(row['text_noSymbHashtags']) for index, row in train_df.iterrows()]\n",
    "\n",
    "#train_df['text_noEm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing\n",
    "text_noMentions=[]\n",
    "text_noHashtags=[]\n",
    "text_spanish=[]\n",
    "for index, row in train_df.iterrows():\n",
    "    text_noMentions.append(filter_mentions(row['text'])) # text without mentions\n",
    "    text_noHashtags.append(filter_hashtags(row['text'])) # text without hahstags\n",
    "    text_spanish.append(translate_text(row['text']).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>username</th>\n",
       "      <th>party</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>text_noEm</th>\n",
       "      <th>text_noSymbHashtags</th>\n",
       "      <th>text_noSymbHashtags_more</th>\n",
       "      <th>language</th>\n",
       "      <th>lang_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>martarovira</td>\n",
       "      <td>erc</td>\n",
       "      <td>√öltim acte de campanya! Aqu√≠ tossudament al√ßat...</td>\n",
       "      <td>2017-12-19 20:12:01</td>\n",
       "      <td>785</td>\n",
       "      <td>2295</td>\n",
       "      <td>√öltim acte de campanya! Aqu√≠ tossudament al√ßat...</td>\n",
       "      <td>√öltim acte de campanya! Aqu√≠ tossudament al√ßat...</td>\n",
       "      <td>√∫ltim acte de campanya! aqu√≠ tossudament al√ßat...</td>\n",
       "      <td>ca</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>xavierdomenechs</td>\n",
       "      <td>comuns</td>\n",
       "      <td>#Badalona necessita uns pressupostos que posin...</td>\n",
       "      <td>2018-04-27 10:04:19</td>\n",
       "      <td>55</td>\n",
       "      <td>93</td>\n",
       "      <td>#Badalona necessita uns pressupostos que posin...</td>\n",
       "      <td>Badalona necessita uns pressupostos que posin ...</td>\n",
       "      <td>badalona necessita uns pressupostos que posin ...</td>\n",
       "      <td>ca</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>albert_rivera</td>\n",
       "      <td>cs</td>\n",
       "      <td>Encuentro Villac√≠s-Valls para lanzar una estra...</td>\n",
       "      <td>2018-11-17 20:34:58</td>\n",
       "      <td>357</td>\n",
       "      <td>622</td>\n",
       "      <td>Encuentro Villac√≠s-Valls para lanzar una estra...</td>\n",
       "      <td>Encuentro Villac√≠s-Valls para lanzar una estra...</td>\n",
       "      <td>encuentro villac√≠s-valls para lanzar una estra...</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>jaumecollboni</td>\n",
       "      <td>psc</td>\n",
       "      <td>‚ÄúLa palabra es como una bala, no tiene retorno...</td>\n",
       "      <td>2018-10-22 18:10:01</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>‚ÄúLa palabra es como una bala, no tiene retorno...</td>\n",
       "      <td>‚ÄúLa palabra es como una bala, no tiene retorno...</td>\n",
       "      <td>‚Äúla palabra es como una bala, no tiene retorno...</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>albiol_xg</td>\n",
       "      <td>ppc</td>\n",
       "      <td>üìª Esta noche, a partir de las 22:10h, me entre...</td>\n",
       "      <td>2018-08-16 10:30:27</td>\n",
       "      <td>20</td>\n",
       "      <td>47</td>\n",
       "      <td>Esta noche, a partir de las 22:10h, me entrev...</td>\n",
       "      <td>üìª Esta noche, a partir de las 22:10h, me entre...</td>\n",
       "      <td>üìª esta noche, a partir de las 22:10h, me entre...</td>\n",
       "      <td>es</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id         username   party  \\\n",
       "0   0      martarovira     erc   \n",
       "1   1  xavierdomenechs  comuns   \n",
       "2   2    albert_rivera      cs   \n",
       "3   3    jaumecollboni     psc   \n",
       "4   4        albiol_xg     ppc   \n",
       "\n",
       "                                                text          created_at  \\\n",
       "0  √öltim acte de campanya! Aqu√≠ tossudament al√ßat... 2017-12-19 20:12:01   \n",
       "1  #Badalona necessita uns pressupostos que posin... 2018-04-27 10:04:19   \n",
       "2  Encuentro Villac√≠s-Valls para lanzar una estra... 2018-11-17 20:34:58   \n",
       "3  ‚ÄúLa palabra es como una bala, no tiene retorno... 2018-10-22 18:10:01   \n",
       "4  üìª Esta noche, a partir de las 22:10h, me entre... 2018-08-16 10:30:27   \n",
       "\n",
       "   retweet_count  favorite_count  \\\n",
       "0            785            2295   \n",
       "1             55              93   \n",
       "2            357             622   \n",
       "3              4               6   \n",
       "4             20              47   \n",
       "\n",
       "                                           text_noEm  \\\n",
       "0  √öltim acte de campanya! Aqu√≠ tossudament al√ßat...   \n",
       "1  #Badalona necessita uns pressupostos que posin...   \n",
       "2  Encuentro Villac√≠s-Valls para lanzar una estra...   \n",
       "3  ‚ÄúLa palabra es como una bala, no tiene retorno...   \n",
       "4   Esta noche, a partir de las 22:10h, me entrev...   \n",
       "\n",
       "                                 text_noSymbHashtags  \\\n",
       "0  √öltim acte de campanya! Aqu√≠ tossudament al√ßat...   \n",
       "1  Badalona necessita uns pressupostos que posin ...   \n",
       "2  Encuentro Villac√≠s-Valls para lanzar una estra...   \n",
       "3  ‚ÄúLa palabra es como una bala, no tiene retorno...   \n",
       "4  üìª Esta noche, a partir de las 22:10h, me entre...   \n",
       "\n",
       "                            text_noSymbHashtags_more language  lang_val  \n",
       "0  √∫ltim acte de campanya! aqu√≠ tossudament al√ßat...       ca         1  \n",
       "1  badalona necessita uns pressupostos que posin ...       ca         1  \n",
       "2  encuentro villac√≠s-valls para lanzar una estra...       es         0  \n",
       "3  ‚Äúla palabra es como una bala, no tiene retorno...       es         0  \n",
       "4  üìª esta noche, a partir de las 22:10h, me entre...       es         0  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Translate text to spanish and define column (vector) with language (spa:0, cat:1)\n",
    "# !pip install langdetect\n",
    "from langdetect import detect \n",
    "train_df['language'] = [detect(row['text']) for index, row in train_df.iterrows()]\n",
    "\n",
    "# language column to 1-0 --> esta columna lang_val la ponemos al final del array (Xvec)\n",
    "train_df['lang_val'] = np.where(train_df.language == 'ca', 1, 0)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translate to spanish (a vegades funciona) --> putada no podem fer-ho servir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify hashtags? fa falta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la part de lemmatizer --> # No esta per catala --> sino el podem traduir, no ho podem fer servir\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "#wordnet_lemmatizer = WordNetLemmatizer()\n",
    "#wordnet_lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC with tfidf ( THIS IS THE GOOD ONE....) !!!\n",
    "min_df: minimum number of documents that should contain this feature.\n",
    "max_df: Here 0.7 means that we should include only those words that occur in a maximum of 70% of all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1920, 6633)\n"
     ]
    }
   ],
   "source": [
    "# create the Vector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#vectorizer = TfidfVectorizer(max_df=0.50,stop_words=stop_words(),ngram_range=(1, 4),min_df=0.0001,max_features=12000)\n",
    "vectorizer = TfidfVectorizer(max_df=0.50,stop_words=stop_words(),ngram_range=(1, 4),min_df=2, max_features=8000)\n",
    "#vectorizer = TfidfVectorizer(max_df=0.80,stop_words=stop_words(),ngram_range=(1, 6),min_df=0.0001,max_features=12000)\n",
    "#Xvec = vectorizer.fit_transform(train_df['text_noEm_noSymbHashtags']).toarray()\n",
    "#Xvec = vectorizer.fit_transform(train_df['text_noEm']).toarray()\n",
    "#Xvec = vectorizer.fit_transform(train_df['text_noEm_noHashtags']).toarray()\n",
    "Xvec = vectorizer.fit_transform(train_df['text_noSymbHashtags_more']).toarray()\n",
    "#Xvec = vectorizer.fit_transform(train_df['text_noSymbHashtags']).toarray()\n",
    "#Xvec = vectorizer.fit_transform(text_noHashtags).toarray()\n",
    "\n",
    "yvec = train_df['party'].values\n",
    "print(Xvec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amb aixo afegim la columna de 0,1 a Xvec en funcio idioma a Xvec_lang\n",
    "a,b = Xvec.shape\n",
    "Xvec_lang=np.zeros((a,b+1))\n",
    "Xvec_lang[:,:-1]=Xvec\n",
    "Xvec_lang[:,-1]=train_df['lang_val']\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_train:  =  1.0\n",
      "pred_test: =  0.5234375\n",
      "[[28  5  4 16 11  7]\n",
      " [ 6 40  0  0 11  4]\n",
      " [ 5  0 29 21  6  4]\n",
      " [ 9  1 16 28  9  3]\n",
      " [ 2  3  1  3 46  5]\n",
      " [ 6  2  5  9  9 30]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     comuns       0.50      0.39      0.44        71\n",
      "         cs       0.78      0.66      0.71        61\n",
      "        erc       0.53      0.45      0.48        65\n",
      "      jxcat       0.36      0.42      0.39        66\n",
      "        ppc       0.50      0.77      0.61        60\n",
      "        psc       0.57      0.49      0.53        61\n",
      "\n",
      "avg / total       0.54      0.52      0.52       384\n",
      "\n",
      "0.5234375\n"
     ]
    }
   ],
   "source": [
    "# The model Linear SVC: Linear Support Vector Classification\n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "# Split train and test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(Xvec, yvec, test_size=0.20)\n",
    "pred_train = [] \n",
    "pred_test = [] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xvec, yvec, test_size=0.20, random_state=345 )\n",
    "\n",
    "\n",
    "# My Model\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "prediction_train = clf.predict(X_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "pred_train = (np.mean([prediction_train == y_train]))\n",
    "pred_test =(np.mean([prediction == y_test])) \n",
    "\n",
    "\n",
    "print('pred_train:  = ', pred_train)\n",
    "print('pred_test: = ',pred_test )\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,prediction))  \n",
    "print(classification_report(y_test,prediction))  \n",
    "print(accuracy_score(y_test, prediction))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['jxcat', 'psc', 'cs', ..., 'erc', 'ppc', 'psc'], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.72\n",
      "{'clf__C': 1.0, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 1, 'clf__penalty': 'l2', 'vect__max_features': 8000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estar√°', 'estar√°n', 'estar√°s', 'estar√©', 'estar√©is', 'estar√≠a', 'estar√≠ais', 'estar√≠amos', 'estar√≠an', 'estar√≠as', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuvi√©ramos', 'estuvi√©semos', 'estuvo', 'est√°', 'est√°bamos', 'est√°is', 'est√°n', 'est√°s', 'est√©', 'est√©is', 'est√©n', 'est√©s', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fu√©ramos', 'fu√©semos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habr√°', 'habr√°n', 'habr√°s', 'habr√©', 'habr√©is', 'habr√≠a', 'habr√≠ais', 'habr√≠amos', 'habr√≠an', 'habr√≠as', 'hab√©is', 'hab√≠a', 'hab√≠ais', 'hab√≠amos', 'hab√≠an', 'hab√≠as', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hay√°is', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubi√©ramos', 'hubi√©semos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'm√°s', 'm√≠', 'm√≠a', 'm√≠as', 'm√≠o', 'm√≠os', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque', 'que', 'quien', 'quienes', 'qu√©', 'se', 'sea', 'seamos', 'sean', 'seas', 'seremos', 'ser√°', 'ser√°n', 'ser√°s', 'ser√©', 'ser√©is', 'ser√≠a', 'ser√≠ais', 'ser√≠amos', 'ser√≠an', 'ser√≠as', 'se√°is', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 's√≠', 'tambi√©n', 'tanto', 'te', 'tendremos', 'tendr√°', 'tendr√°n', 'tendr√°s', 'tendr√©', 'tendr√©is', 'tendr√≠a', 'tendr√≠ais', 'tendr√≠amos', 'tendr√≠an', 'tendr√≠as', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'teng√°is', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'ten√©is', 'ten√≠a', 'ten√≠ais', 'ten√≠amos', 'ten√≠an', 'ten√≠as', 'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuvi√©ramos', 'tuvi√©semos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 't√∫', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', '√©l', '√©ramos', '\\ufeffa', 'abans', 'algun', 'alguna', 'algunes', 'alguns', 'altre', 'amb', 'ambd√≥s', 'anar', 'ans', 'aquell', 'aquelles', 'aquells', 'aqu√≠', 'bastant', 'b√©', 'cada', 'com', 'consegueixo', 'conseguim', 'conseguir', 'consigueix', 'consigueixen', 'consigueixes', 'dalt', 'de', 'des de', 'dins', 'el', 'elles', 'ells', 'els', 'en', 'ens', 'entre', 'era', 'erem', 'eren', 'eres', 'es', '√©s', '√©ssent', 'est√†', 'estan', 'estat', 'estava', 'estem', 'esteu', 'estic', 'ets', 'fa', 'faig', 'fan', 'fas', 'fem', 'fer', 'feu', 'fi', 'haver', 'i', 'incl√≤s', 'jo', 'la', 'les', 'llarg', 'llavors', 'mentre', 'meu', 'mode', 'molt', 'molts', 'nosaltres', 'o', 'on', 'per', 'per', 'per que', 'per√≤', 'perqu√®', 'podem', 'poden', 'poder', 'podeu', 'potser', 'primer', 'puc', 'quan', 'quant', 'qui', 'sabem', 'saben', 'saber', 'sabeu', 'sap', 'saps', 'sense', 'ser', 'seu', 'seus', 'si', 'soc', 'solament', 'sols', 'som', 'sota', 'tamb√©', 'te', 'tene', 'tenim', 'tenir', 'teniu', 'teu', 'tinc', 'tot', '√∫ltim', 'un', 'un', 'una', 'unes', 'uns', '√∫s', 'va', 'vaig', 'van', 'vosaltres', '', 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']}\n"
     ]
    }
   ],
   "source": [
    "# http://queirozf.com/entries/scikit-learn-pipeline-examples#pipeline-for-text-classification-nlp\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', LinearSVC()),\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text_noSymbHashtags_more'], train_df['party'].values, test_size=0.20, random_state=345 )\n",
    "param_grid = [\n",
    "    { \n",
    "          \"clf__penalty\": [\"l2\"],\n",
    "          \"clf__dual\":[False,True],\n",
    "          \"clf__C\":[0.1, 1.0],\n",
    "          \"clf__fit_intercept\":[True,False],\n",
    "          \"clf__intercept_scaling\":[0.1,1,10,100],\n",
    "          \"vect__min_df\":[1,2,3,4],\n",
    "          \"vect__ngram_range\": [(1, 1), (1, 2),(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000, 8000, 10000]\n",
    "    },\n",
    "    { \n",
    "          \"clf__penalty\": [\"l1\"],\n",
    "          \"clf__dual\":[False],\n",
    "          \"clf__C\":[0.1, 1.0],\n",
    "          \"clf__fit_intercept\":[True,False],\n",
    "          \"clf__intercept_scaling\":[0.1,1,10,100],\n",
    "          \"vect__min_df\":[1,2,3,4],\n",
    "          \"vect__ngram_range\": [(1, 1),(1, 2),(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000, 8000, 10000]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=6, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print (\"score = %3.2f\" %(grid.score(X_test,y_test)) )\n",
    "print (grid.best_params_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scores\n",
    "\n",
    "# LinearSVC\n",
    "\n",
    "## test1 \n",
    "score = 0.72  \n",
    "{'clf__C': 1.0, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 1, 'clf__penalty': 'l2', 'vect__max_features': 8000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words':\n",
    "\n",
    "# Naive Bayes (Multinomial)\n",
    "score = 0.67\n",
    "{'clf__alpha': 0.1, 'vect__max_features': 6000, 'vect__min_df': 1, 'vect__ngram_range': (1, 3), 'vect__stop_words'\n",
    "\n",
    "# LogisticRegression\n",
    "score = 0.72\n",
    "{'clf__C': 10, 'clf__dual': True, 'clf__fit_intercept': False, 'clf__penalty': 'l2', 'clf__solver': 'liblinear', 'clf__tol': 0.0001, 'vect__max_features': 6000, 'vect__min_df': 1, 'vect__ngram_range': (1, 3), 'vect__stop_words':\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.67\n",
      "{'clf__alpha': 0.1, 'vect__max_features': 6000, 'vect__min_df': 1, 'vect__ngram_range': (1, 3), 'vect__stop_words': ['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estar√°', 'estar√°n', 'estar√°s', 'estar√©', 'estar√©is', 'estar√≠a', 'estar√≠ais', 'estar√≠amos', 'estar√≠an', 'estar√≠as', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuvi√©ramos', 'estuvi√©semos', 'estuvo', 'est√°', 'est√°bamos', 'est√°is', 'est√°n', 'est√°s', 'est√©', 'est√©is', 'est√©n', 'est√©s', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fu√©ramos', 'fu√©semos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habr√°', 'habr√°n', 'habr√°s', 'habr√©', 'habr√©is', 'habr√≠a', 'habr√≠ais', 'habr√≠amos', 'habr√≠an', 'habr√≠as', 'hab√©is', 'hab√≠a', 'hab√≠ais', 'hab√≠amos', 'hab√≠an', 'hab√≠as', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hay√°is', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubi√©ramos', 'hubi√©semos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'm√°s', 'm√≠', 'm√≠a', 'm√≠as', 'm√≠o', 'm√≠os', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque', 'que', 'quien', 'quienes', 'qu√©', 'se', 'sea', 'seamos', 'sean', 'seas', 'seremos', 'ser√°', 'ser√°n', 'ser√°s', 'ser√©', 'ser√©is', 'ser√≠a', 'ser√≠ais', 'ser√≠amos', 'ser√≠an', 'ser√≠as', 'se√°is', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 's√≠', 'tambi√©n', 'tanto', 'te', 'tendremos', 'tendr√°', 'tendr√°n', 'tendr√°s', 'tendr√©', 'tendr√©is', 'tendr√≠a', 'tendr√≠ais', 'tendr√≠amos', 'tendr√≠an', 'tendr√≠as', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'teng√°is', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'ten√©is', 'ten√≠a', 'ten√≠ais', 'ten√≠amos', 'ten√≠an', 'ten√≠as', 'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuvi√©ramos', 'tuvi√©semos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 't√∫', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', '√©l', '√©ramos', '\\ufeffa', 'abans', 'algun', 'alguna', 'algunes', 'alguns', 'altre', 'amb', 'ambd√≥s', 'anar', 'ans', 'aquell', 'aquelles', 'aquells', 'aqu√≠', 'bastant', 'b√©', 'cada', 'com', 'consegueixo', 'conseguim', 'conseguir', 'consigueix', 'consigueixen', 'consigueixes', 'dalt', 'de', 'des de', 'dins', 'el', 'elles', 'ells', 'els', 'en', 'ens', 'entre', 'era', 'erem', 'eren', 'eres', 'es', '√©s', '√©ssent', 'est√†', 'estan', 'estat', 'estava', 'estem', 'esteu', 'estic', 'ets', 'fa', 'faig', 'fan', 'fas', 'fem', 'fer', 'feu', 'fi', 'haver', 'i', 'incl√≤s', 'jo', 'la', 'les', 'llarg', 'llavors', 'mentre', 'meu', 'mode', 'molt', 'molts', 'nosaltres', 'o', 'on', 'per', 'per', 'per que', 'per√≤', 'perqu√®', 'podem', 'poden', 'poder', 'podeu', 'potser', 'primer', 'puc', 'quan', 'quant', 'qui', 'sabem', 'saben', 'saber', 'sabeu', 'sap', 'saps', 'sense', 'ser', 'seu', 'seus', 'si', 'soc', 'solament', 'sols', 'som', 'sota', 'tamb√©', 'te', 'tene', 'tenim', 'tenir', 'teniu', 'teu', 'tinc', 'tot', '√∫ltim', 'un', 'un', 'una', 'unes', 'uns', '√∫s', 'va', 'vaig', 'van', 'vosaltres', '', 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text_noSymbHashtags_more'], train_df['party'].values, test_size=0.20, random_state=345 )\n",
    "param_grid = [\n",
    "    { \n",
    "          \"clf__alpha\": [0.001,0.01, 0.1, 1, 10, 100],\n",
    "          \"vect__min_df\":[1,2,3,4],\n",
    "          \"vect__ngram_range\": [(1, 1), (1, 2),(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000, 8000, 10000]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=6, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print (\"score = %3.2f\" %(grid.score(X_test,y_test)) )\n",
    "print (grid.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "C:\\Users\\Pablo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.72\n",
      "{'clf__C': 10, 'clf__dual': True, 'clf__fit_intercept': False, 'clf__penalty': 'l2', 'clf__solver': 'liblinear', 'clf__tol': 0.0001, 'vect__max_features': 6000, 'vect__min_df': 1, 'vect__ngram_range': (1, 3), 'vect__stop_words': ['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estar√°', 'estar√°n', 'estar√°s', 'estar√©', 'estar√©is', 'estar√≠a', 'estar√≠ais', 'estar√≠amos', 'estar√≠an', 'estar√≠as', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuvi√©ramos', 'estuvi√©semos', 'estuvo', 'est√°', 'est√°bamos', 'est√°is', 'est√°n', 'est√°s', 'est√©', 'est√©is', 'est√©n', 'est√©s', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fu√©ramos', 'fu√©semos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habr√°', 'habr√°n', 'habr√°s', 'habr√©', 'habr√©is', 'habr√≠a', 'habr√≠ais', 'habr√≠amos', 'habr√≠an', 'habr√≠as', 'hab√©is', 'hab√≠a', 'hab√≠ais', 'hab√≠amos', 'hab√≠an', 'hab√≠as', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hay√°is', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubi√©ramos', 'hubi√©semos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'm√°s', 'm√≠', 'm√≠a', 'm√≠as', 'm√≠o', 'm√≠os', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque', 'que', 'quien', 'quienes', 'qu√©', 'se', 'sea', 'seamos', 'sean', 'seas', 'seremos', 'ser√°', 'ser√°n', 'ser√°s', 'ser√©', 'ser√©is', 'ser√≠a', 'ser√≠ais', 'ser√≠amos', 'ser√≠an', 'ser√≠as', 'se√°is', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 's√≠', 'tambi√©n', 'tanto', 'te', 'tendremos', 'tendr√°', 'tendr√°n', 'tendr√°s', 'tendr√©', 'tendr√©is', 'tendr√≠a', 'tendr√≠ais', 'tendr√≠amos', 'tendr√≠an', 'tendr√≠as', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'teng√°is', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'ten√©is', 'ten√≠a', 'ten√≠ais', 'ten√≠amos', 'ten√≠an', 'ten√≠as', 'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuvi√©ramos', 'tuvi√©semos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 't√∫', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', '√©l', '√©ramos', '\\ufeffa', 'abans', 'algun', 'alguna', 'algunes', 'alguns', 'altre', 'amb', 'ambd√≥s', 'anar', 'ans', 'aquell', 'aquelles', 'aquells', 'aqu√≠', 'bastant', 'b√©', 'cada', 'com', 'consegueixo', 'conseguim', 'conseguir', 'consigueix', 'consigueixen', 'consigueixes', 'dalt', 'de', 'des de', 'dins', 'el', 'elles', 'ells', 'els', 'en', 'ens', 'entre', 'era', 'erem', 'eren', 'eres', 'es', '√©s', '√©ssent', 'est√†', 'estan', 'estat', 'estava', 'estem', 'esteu', 'estic', 'ets', 'fa', 'faig', 'fan', 'fas', 'fem', 'fer', 'feu', 'fi', 'haver', 'i', 'incl√≤s', 'jo', 'la', 'les', 'llarg', 'llavors', 'mentre', 'meu', 'mode', 'molt', 'molts', 'nosaltres', 'o', 'on', 'per', 'per', 'per que', 'per√≤', 'perqu√®', 'podem', 'poden', 'poder', 'podeu', 'potser', 'primer', 'puc', 'quan', 'quant', 'qui', 'sabem', 'saben', 'saber', 'sabeu', 'sap', 'saps', 'sense', 'ser', 'seu', 'seus', 'si', 'soc', 'solament', 'sols', 'som', 'sota', 'tamb√©', 'te', 'tene', 'tenim', 'tenir', 'teniu', 'teu', 'tinc', 'tot', '√∫ltim', 'un', 'un', 'una', 'unes', 'uns', '√∫s', 'va', 'vaig', 'van', 'vosaltres', '', 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']}\n"
     ]
    }
   ],
   "source": [
    "# class sklearn.linear_model.LogisticRegression(penalty=‚Äôl2‚Äô,\n",
    "#dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, \n",
    "#random_state=None, solver=‚Äôwarn‚Äô, max_iter=100, multi_class=‚Äôwarn‚Äô, verbose=0, warm_start=False, n_jobs=None)[source]¬∂\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text_noSymbHashtags_more'], train_df['party'].values, test_size=0.20, random_state=345 )\n",
    "param_grid = [\n",
    "    { \n",
    "          \"clf__penalty\": [\"l2\"],\n",
    "          \"clf__dual\":[True],\n",
    "          \"clf__solver\":['liblinear'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":[0.1,1,10],\n",
    "          \"clf__fit_intercept\":[True, False],\n",
    "          \"vect__min_df\":[1],\n",
    "          \"vect__ngram_range\": [(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000]\n",
    "    },\n",
    "    { \n",
    "          \"clf__penalty\": [\"l2\"],\n",
    "          \"clf__dual\":[False],\n",
    "          \"clf__solver\":['lbfgs','newton-cg','sag'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":[0.1,1,10],\n",
    "          \"clf__fit_intercept\":[True, False],\n",
    "          \"vect__min_df\":[1],\n",
    "          \"vect__ngram_range\": [(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000]\n",
    "    },\n",
    "    { \n",
    "          \"clf__penalty\": [\"l1\"],\n",
    "          \"clf__dual\":[False],\n",
    "          \"clf__solver\":['liblinear', 'saga'],\n",
    "          \"clf__tol\":[0.0001, 0.001, 0.01],\n",
    "          \"clf__C\":[0.1,1,10],\n",
    "          \"clf__fit_intercept\":[True, False],\n",
    "          \"vect__min_df\":[1],\n",
    "          \"vect__ngram_range\": [(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000]\n",
    "    }\n",
    "]\n",
    "grid = GridSearchCV(pipeline, cv=6, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print (\"score = %3.2f\" %(grid.score(X_test,y_test)) )\n",
    "print (grid.best_params_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.72\n",
      "{'clf__cv': 5, 'vect__max_features': 6000, 'vect__min_df': 1, 'vect__ngram_range': (1, 1), 'vect__stop_words': ['a', 'al', 'algo', 'algunas', 'algunos', 'ante', 'antes', 'como', 'con', 'contra', 'cual', 'cuando', 'de', 'del', 'desde', 'donde', 'durante', 'e', 'el', 'ella', 'ellas', 'ellos', 'en', 'entre', 'era', 'erais', 'eran', 'eras', 'eres', 'es', 'esa', 'esas', 'ese', 'eso', 'esos', 'esta', 'estaba', 'estabais', 'estaban', 'estabas', 'estad', 'estada', 'estadas', 'estado', 'estados', 'estamos', 'estando', 'estar', 'estaremos', 'estar√°', 'estar√°n', 'estar√°s', 'estar√©', 'estar√©is', 'estar√≠a', 'estar√≠ais', 'estar√≠amos', 'estar√≠an', 'estar√≠as', 'estas', 'este', 'estemos', 'esto', 'estos', 'estoy', 'estuve', 'estuviera', 'estuvierais', 'estuvieran', 'estuvieras', 'estuvieron', 'estuviese', 'estuvieseis', 'estuviesen', 'estuvieses', 'estuvimos', 'estuviste', 'estuvisteis', 'estuvi√©ramos', 'estuvi√©semos', 'estuvo', 'est√°', 'est√°bamos', 'est√°is', 'est√°n', 'est√°s', 'est√©', 'est√©is', 'est√©n', 'est√©s', 'fue', 'fuera', 'fuerais', 'fueran', 'fueras', 'fueron', 'fuese', 'fueseis', 'fuesen', 'fueses', 'fui', 'fuimos', 'fuiste', 'fuisteis', 'fu√©ramos', 'fu√©semos', 'ha', 'habida', 'habidas', 'habido', 'habidos', 'habiendo', 'habremos', 'habr√°', 'habr√°n', 'habr√°s', 'habr√©', 'habr√©is', 'habr√≠a', 'habr√≠ais', 'habr√≠amos', 'habr√≠an', 'habr√≠as', 'hab√©is', 'hab√≠a', 'hab√≠ais', 'hab√≠amos', 'hab√≠an', 'hab√≠as', 'han', 'has', 'hasta', 'hay', 'haya', 'hayamos', 'hayan', 'hayas', 'hay√°is', 'he', 'hemos', 'hube', 'hubiera', 'hubierais', 'hubieran', 'hubieras', 'hubieron', 'hubiese', 'hubieseis', 'hubiesen', 'hubieses', 'hubimos', 'hubiste', 'hubisteis', 'hubi√©ramos', 'hubi√©semos', 'hubo', 'la', 'las', 'le', 'les', 'lo', 'los', 'me', 'mi', 'mis', 'mucho', 'muchos', 'muy', 'm√°s', 'm√≠', 'm√≠a', 'm√≠as', 'm√≠o', 'm√≠os', 'nada', 'ni', 'no', 'nos', 'nosotras', 'nosotros', 'nuestra', 'nuestras', 'nuestro', 'nuestros', 'o', 'os', 'otra', 'otras', 'otro', 'otros', 'para', 'pero', 'poco', 'por', 'porque', 'que', 'quien', 'quienes', 'qu√©', 'se', 'sea', 'seamos', 'sean', 'seas', 'seremos', 'ser√°', 'ser√°n', 'ser√°s', 'ser√©', 'ser√©is', 'ser√≠a', 'ser√≠ais', 'ser√≠amos', 'ser√≠an', 'ser√≠as', 'se√°is', 'sido', 'siendo', 'sin', 'sobre', 'sois', 'somos', 'son', 'soy', 'su', 'sus', 'suya', 'suyas', 'suyo', 'suyos', 's√≠', 'tambi√©n', 'tanto', 'te', 'tendremos', 'tendr√°', 'tendr√°n', 'tendr√°s', 'tendr√©', 'tendr√©is', 'tendr√≠a', 'tendr√≠ais', 'tendr√≠amos', 'tendr√≠an', 'tendr√≠as', 'tened', 'tenemos', 'tenga', 'tengamos', 'tengan', 'tengas', 'tengo', 'teng√°is', 'tenida', 'tenidas', 'tenido', 'tenidos', 'teniendo', 'ten√©is', 'ten√≠a', 'ten√≠ais', 'ten√≠amos', 'ten√≠an', 'ten√≠as', 'ti', 'tiene', 'tienen', 'tienes', 'todo', 'todos', 'tu', 'tus', 'tuve', 'tuviera', 'tuvierais', 'tuvieran', 'tuvieras', 'tuvieron', 'tuviese', 'tuvieseis', 'tuviesen', 'tuvieses', 'tuvimos', 'tuviste', 'tuvisteis', 'tuvi√©ramos', 'tuvi√©semos', 'tuvo', 'tuya', 'tuyas', 'tuyo', 'tuyos', 't√∫', 'un', 'una', 'uno', 'unos', 'vosotras', 'vosotros', 'vuestra', 'vuestras', 'vuestro', 'vuestros', 'y', 'ya', 'yo', '√©l', '√©ramos', '\\ufeffa', 'abans', 'algun', 'alguna', 'algunes', 'alguns', 'altre', 'amb', 'ambd√≥s', 'anar', 'ans', 'aquell', 'aquelles', 'aquells', 'aqu√≠', 'bastant', 'b√©', 'cada', 'com', 'consegueixo', 'conseguim', 'conseguir', 'consigueix', 'consigueixen', 'consigueixes', 'dalt', 'de', 'des de', 'dins', 'el', 'elles', 'ells', 'els', 'en', 'ens', 'entre', 'era', 'erem', 'eren', 'eres', 'es', '√©s', '√©ssent', 'est√†', 'estan', 'estat', 'estava', 'estem', 'esteu', 'estic', 'ets', 'fa', 'faig', 'fan', 'fas', 'fem', 'fer', 'feu', 'fi', 'haver', 'i', 'incl√≤s', 'jo', 'la', 'les', 'llarg', 'llavors', 'mentre', 'meu', 'mode', 'molt', 'molts', 'nosaltres', 'o', 'on', 'per', 'per', 'per que', 'per√≤', 'perqu√®', 'podem', 'poden', 'poder', 'podeu', 'potser', 'primer', 'puc', 'quan', 'quant', 'qui', 'sabem', 'saben', 'saber', 'sabeu', 'sap', 'saps', 'sense', 'ser', 'seu', 'seus', 'si', 'soc', 'solament', 'sols', 'som', 'sota', 'tamb√©', 'te', 'tene', 'tenim', 'tenir', 'teniu', 'teu', 'tinc', 'tot', '√∫ltim', 'un', 'un', 'una', 'unes', 'uns', '√∫s', 'va', 'vaig', 'van', 'vosaltres', '', 'a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself', \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where', \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\", 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']}\n",
      "[[38  2  6 13  0 12]\n",
      " [ 0 54  0  0  3  4]\n",
      " [ 2  1 42 17  3  0]\n",
      " [ 3  1 10 49  1  2]\n",
      " [ 0  6  1  1 52  0]\n",
      " [ 5  4  2  5  2 43]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     comuns       0.79      0.54      0.64        71\n",
      "         cs       0.79      0.89      0.84        61\n",
      "        erc       0.69      0.65      0.67        65\n",
      "      jxcat       0.58      0.74      0.65        66\n",
      "        ppc       0.85      0.87      0.86        60\n",
      "        psc       0.70      0.70      0.70        61\n",
      "\n",
      "avg / total       0.73      0.72      0.72       384\n",
      "\n",
      "0.7239583333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "#clf = LogisticRegressionCV(cv=5, random_state=0,multi_class='multinomial').fit(X, y)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegressionCV()),\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text_noSymbHashtags_more'], train_df['party'].values, test_size=0.20, random_state=345 )\n",
    "param_grid = [\n",
    "    { \n",
    "          \"clf__cv\": [2,5,8],\n",
    "          \"vect__min_df\":[1],\n",
    "          \"vect__ngram_range\": [(1,1),(1,2),(1, 3)],\n",
    "          \"vect__stop_words\":[stop_words()],\n",
    "          \"vect__max_features\":[6000]\n",
    "    }\n",
    "]\n",
    "grid = GridSearchCV(pipeline, cv=6, param_grid=param_grid)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print (\"score = %3.2f\" %(grid.score(X_test,y_test)) )\n",
    "print (grid.best_params_ )\n",
    "\n",
    "prediction_train = grid.predict(X_train)\n",
    "prediction = grid.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,prediction))  \n",
    "print(classification_report(y_test,prediction))  \n",
    "print(accuracy_score(y_test, prediction))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36  7  5 14  1  8]\n",
      " [ 0 59  0  0  1  1]\n",
      " [ 4  2 34 23  2  0]\n",
      " [ 3  2  1 56  1  3]\n",
      " [ 1  6  0  2 51  0]\n",
      " [ 7  6  2  5  0 41]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     comuns       0.71      0.51      0.59        71\n",
      "         cs       0.72      0.97      0.83        61\n",
      "        erc       0.81      0.52      0.64        65\n",
      "      jxcat       0.56      0.85      0.67        66\n",
      "        ppc       0.91      0.85      0.88        60\n",
      "        psc       0.77      0.67      0.72        61\n",
      "\n",
      "avg / total       0.74      0.72      0.72       384\n",
      "\n",
      "0.7213541666666666\n",
      "{'clf__alpha': 1e-05, 'clf__max_iter': 50, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 2)}\n",
      "0.7213541666666666\n"
     ]
    }
   ],
   "source": [
    "prediction_train = grid.predict(X_train)\n",
    "prediction = grid.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,prediction))  \n",
    "print(classification_report(y_test,prediction))  \n",
    "print(accuracy_score(y_test, prediction))  \n",
    "\n",
    "print (grid.best_params_ )\n",
    "print (grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32  9  6 15  1  8]\n",
      " [ 0 58  0  0  2  1]\n",
      " [ 1  1 41 21  1  0]\n",
      " [ 3  0 11 50  1  1]\n",
      " [ 0  8  1  2 49  0]\n",
      " [ 6  4  3  8  3 37]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     comuns       0.76      0.45      0.57        71\n",
      "         cs       0.72      0.95      0.82        61\n",
      "        erc       0.66      0.63      0.65        65\n",
      "      jxcat       0.52      0.76      0.62        66\n",
      "        ppc       0.86      0.82      0.84        60\n",
      "        psc       0.79      0.61      0.69        61\n",
      "\n",
      "avg / total       0.72      0.70      0.69       384\n",
      "\n",
      "0.6953125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xvec, yvec, test_size=0.20, random_state=345 )\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "prediction_train = clf.predict(X_train)\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,prediction))  \n",
    "print(classification_report(y_test,prediction))  \n",
    "print(accuracy_score(y_test, prediction))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score = 0.71\n",
      "{'clf__alpha': 1e-05, 'clf__loss': 'hinge', 'clf__max_iter': 80, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 2)}\n",
      "[[39  8  4 14  1  5]\n",
      " [ 0 58  0  0  2  1]\n",
      " [ 5  2 37 20  1  0]\n",
      " [ 4  3  6 50  1  2]\n",
      " [ 0  8  0  2 50  0]\n",
      " [ 7  5  4  5  1 39]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     comuns       0.71      0.55      0.62        71\n",
      "         cs       0.69      0.95      0.80        61\n",
      "        erc       0.73      0.57      0.64        65\n",
      "      jxcat       0.55      0.76      0.64        66\n",
      "        ppc       0.89      0.83      0.86        60\n",
      "        psc       0.83      0.64      0.72        61\n",
      "\n",
      "avg / total       0.73      0.71      0.71       384\n",
      "\n",
      "0.7109375\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text_noSymbHashtags_more'], train_df['party'].values, test_size=0.20, random_state=345 )\n",
    "\n",
    "#clf = SGDClassifier(loss='log', penalty='l1',alpha=1e-6, max_iter=50,random_state=42).fit(X_train, y_train)\n",
    "\n",
    "parameters = {\n",
    "    #'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'),\n",
    "    'clf__alpha': (0.0001, 0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=6, param_grid=parameters)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print (\"score = %3.2f\" %(grid.score(X_test,y_test)) )\n",
    "print (grid.best_params_ )\n",
    "\n",
    "prediction_train = grid.predict(X_train)\n",
    "prediction = grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,prediction))  \n",
    "print(classification_report(y_test,prediction))  \n",
    "print(accuracy_score(y_test, prediction))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_train:  =  1.0\n",
      "pred_test: =  0.7239583333333334\n",
      "[[42  1  7  6  1 14]\n",
      " [ 0 54  0  0  3  4]\n",
      " [ 3  0 40 18  4  0]\n",
      " [ 2  0 15 44  1  4]\n",
      " [ 1  5  1  1 51  1]\n",
      " [ 4  3  1  6  0 47]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     comuns       0.81      0.59      0.68        71\n",
      "         cs       0.86      0.89      0.87        61\n",
      "        erc       0.62      0.62      0.62        65\n",
      "      jxcat       0.59      0.67      0.62        66\n",
      "        ppc       0.85      0.85      0.85        60\n",
      "        psc       0.67      0.77      0.72        61\n",
      "\n",
      "avg / total       0.73      0.72      0.72       384\n",
      "\n",
      "0.7239583333333334\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_df['text_noSymbHashtags_more'], train_df['party'].values, test_size=0.20, random_state=345)\n",
    "\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-5, max_iter=80)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.50,ngram_range=(1, 1),stop_words=stop_words(),min_df=1)\n",
    "\n",
    "\n",
    "Xtrain1 = vectorizer.fit_transform(X_train)\n",
    "ytrain1 = y_train\n",
    "\n",
    "clf.fit(Xtrain1, ytrain1)\n",
    "\n",
    "#X_test = vectorizer.transform(test['text_noSymbHashtags_more']).toarray()\n",
    "Xtest1 = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "prediction_train = clf.predict(Xtrain1)\n",
    "prediction = clf.predict(Xtest1)\n",
    "\n",
    "pred_train = (np.mean([prediction_train == ytrain1]))\n",
    "pred_test =(np.mean([prediction == y_test])) \n",
    "\n",
    "\n",
    "print('pred_train:  = ', pred_train)\n",
    "print('pred_test: = ',pred_test )\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "print(confusion_matrix(y_test,prediction))  \n",
    "print(classification_report(y_test,prediction))  \n",
    "print(accuracy_score(y_test, prediction))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-33d2ff83c97d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m  \u001b[0mprediction_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m  \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \"\"\"\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    298\u001b[0m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# everything is finite; fall back to O(n) space np.isfinite to prevent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# false positives from overflow in sum method.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n\u001b[0m\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py\u001b[0m in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_prod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# The model Linear SVC: Linear Support Vector Classification\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train and test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(Xvec, yvec, test_size=0.20)\n",
    "pred_train = [] \n",
    "pred_test = [] \n",
    "\n",
    "for i in range(50): # entre 10 y 50...aleatorio total.\n",
    " X_train, X_test, y_train, y_test = train_test_split(Xvec, yvec, test_size=0.10)\n",
    "\n",
    "\n",
    "# My Model\n",
    " clf = LinearSVC(dual=False)\n",
    " clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    " prediction_train = clf.predict(X_train)\n",
    " prediction = clf.predict(X_test)\n",
    "\n",
    " pred_train.append(np.mean([prediction_train == y_train]))\n",
    " pred_test.append(np.mean([prediction == y_test])) \n",
    "\n",
    "\n",
    "plt.plot(pred_train)\n",
    "plt.plot(pred_test)\n",
    "plt.title('Linear SVC with Tdfidf and stop words')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pred_test)\n",
    "print('pred_train: mean = ', np.mean(pred_train), ' max = ', np.max(pred_train),  ' min = ', np.min(pred_train), )\n",
    "print('pred_test: mean = ', np.mean(pred_test), ' max = ', np.max(pred_test), ' min = ', np.min(pred_test), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VOX1+PHPyUbCEtawB9mRVdCIVVTQuuBSl1q3itUu6re/Wm1rv612tXSz/drN1m9brda6VLRo/WKldWdp3QgisiM7CVuAkABJyHZ+f5w7MAyTZJJMmCRz3q9XXpm59869z8zcOfe553nuc0VVcc45lxxSEl0A55xzx48HfeecSyIe9J1zLol40HfOuSTiQd8555KIB33nnEsiHvRbgIjcICKvNPG1K0RkWpyL1OqJyD9F5KbWun0ReUxEftRC275XRJ4Me36liGwVkQMiMqm+fUJEpolIQdjzUSKyRET2i8gdLVHesG3dLCL/bslttGYisklEzkt0ORor6YN+S3xxqvqUql4Qw7aPCSSqOlZV5zVmeyIyWEQ0CBIHgvd0dyOLnVCqepGq/qU1bL85wUxEzgr7Hg5GfC8HRGRQDKu5H7hdVTur6pJG7hPfAOapahdVfaAp76Glicg8EflCosuRrNISXQAXV91UtVpE8oD5IrJYVV+N5wZEJE1Vq+O5zvZEVRcCncEOxsBGgu+lEas5AVjRxCKcAMxq4mtdFO1tn0/6mn59ROQWEVknIntFZI6I9A+bd4GIrBGREhH5XxGZH6q9hNcUxfxKRHYFy34oIuNE5FbgBuAbQQ3wxWD5w2ceIpIqIt8SkfXB6fpiEcltqNyqmo8FjYlh5e0vIs+JSJGIbAw/9ReRLBH5i4gUi8gqEflGRMpgk4h8U0Q+BA6KSFoD65ssIvkiUioiO0Xkl8H0TBF5UkT2iMg+EVkkIn2CeYdrfyKSIiLfEZHNwef2uIh0DeaFzmpuEpEtIrJbRL5dx/c3JNhOSvD8TyKyK2z+kyLylfDti8ho4A/A6cH3si9sld1F5KXgu3hXRIY19F3UU675wXpeBXoF0zuIyAEgFVgqIuvDPv/QPpEldoZYLCIrgVPD1vsGcA7wu6DsI6Ns+7PBd7xfRDaIyG1h86aJSIGI3BV87ttF5LNh83sGv4NSEXkPqPP91/Vdi8iPgbPCyvi7YPkzgmVKgv9nhK1rnoj8VETeC+b/n4j0qGO780XkquDxmcG+cnHw/DwR+SB4HMs+9nkR2QK8EUy/MVh+T+Q+V9c+3yqpalL/AZuA86JMPxfYDZwMdAB+CywI5vUCSoFPYmdLdwJVwBeC+TcD/w4eXwgsBroBAowG+gXzHgN+VFd5gP8GlgGjgteeBPSMUtbBgAJpwfOPAWXAlcHzlKAM3wMygKHABuDCYP59wHygOzAQ+BAoiCjTB0AukBXD+t4GbgwedwY+Fjy+DXgR6IgFtlOA7GDevLDP73PAumC9nYHngSci3uvDQVlOAg4Bo+v4frcApwSP1wTlHB02b1KU7R/+/sLW8xiwF5gcfOdPAbMa2LeO+l7Cpr8N/BLbr84G9gNPhs1XYHgd+8R9wEKgR/B9LI/4rg6/jzrKdAkWrAWYGuwnJwfzpgHVwEwgHbg4mN89mD8LeBboBIwDCiM/p7DtxPRdB897AMXAjcFne33wvGfY8oXBNjsBz4V/XhHbnQn8Nnj8LWA98LOweb9pxD72eLC9LGAMcCD4vjoE31912PcSdZ9vjX9e06/bDcCjqvq+qh4C7sFqf4OxH8MKVX1e7bTvAWBHHeupAroAJwKiqqtUdXuMZfgC8B1VXaNmqaruqWf53SJSju2A/wu8EEw/FchR1ZmqWqmqG7CgeV0w/xrgJ6parKoFwfuJ9ICqblXV8hjWVwUMF5FeqnpAVd8Jm94TC2g1qrpYVUujbOsG4JequkFVD2Cf/XUiEp6O/IGqlqvqUmApFvyjmQ9MFZG+wfPZwfMhQHbw2lg9r6rvBd/5U4SdScVKLKd/KvBdVT2kqguw4Bira4Afq+peVd1K9O+qTqr6kqquD/an+cArWM07pAqYqapVqjoXC3SjRCQVuAr4nqoeVNXlQH1tMLF+12AHoo9U9QlVrVbVp4HVwCfClnlCVZer6kHgu8A1QZkizccOZmAB+qdhz6cG8yG2feze4L2WA58C/qGqC4J48F2gNuL9RtvnWx0P+nXrD2wOPQl2jD3AgGDe1rB5ChREriCY9wbwO+BBYKeIPCQi2TGWIRerqcSqF1bL+DpWa0sPpp8A9A9Os/cFKYtvAX2C+Ue9n4jH0aY1tL7PAyOB1cGp+qXB9CeAl4FZIrJNRH4uIukc66jPPnicFrZ+OPogWxa872jmY5/F2cACrNY4NfhbqKq1dbwumli3WZ/+QHEQvEI217VwHa8P/y4a81pE5CIReUcsZbkPq8D0Cltkjx6dvw69zxzsO4h127F+13Ds9x1a94Cw55HbTY8od8jbwEixtOFErLaeKyK9sLO0BXVsM9o+Fr7NyN/8QSwehNS1z7c6HvTrtg0LbgCISCes5lIIbMfSIKF5Ev48kqo+oKqnAGOxHeO/Q7MaKMNW6smb1rGtGlX9BVAB/L+w9WxU1W5hf11U9eJg/lHvBzvYHLPqiHLVuT5V/UhVrwd6Az8DZotIp6D2+ANVHQOcAVwKfCbKto767IFB2Kn0zkZ8FCHzsZrstODxv4EpHF3rq++9xtt2rG2gU9i0WHr0hL8+/PuJ+bUi0gFLjdwP9FHVbsBcLNXTkCLsO4hp2w1815Gfb+T3HVp3YdjzyO1WYenXyO2WYanHO4HlqloJvAV8DVivqqHXxLKPhZfzqM9dRDpi8SC03aj7fGT5WgMP+iY9aHgK/aUBfwU+KyITgx/LT4B3VXUT8BIwXkSuCJb9EtA32opF5FQROS2o5RzEgnFNMHsnllOsy5+AH4rICDETRKRnPcuHuw9rJM4E3gNKxRpjs8QaiMeJSKgR8FngHhHpLiIDgNsbWHe96xORGSKSE9SiQw2hNSJyjoiMD07LS7Efbk2U9T8NfFWswbMz9tk/o03oQaGqHwHlwAysTaYU+9yvou6gvxMYKCIZjd1eDOXZDOQDPxCRDBE5k6PTGA0J/64GAl9uxGszsHx0EVAtIhcBDXYtDspdg+W97xWRjiIyBqjvuob6vuvI/X4uVjv/tFgngWuxHPo/wpaZISJjgmA7E5gdlCma+dg+HPp+50U8h8bvY7OBS8UahzOCMhyOn3Xt83WsK6E86Ju5WGAI/d2rqq9jebvnsKP8MIKcdVBbuBr4OXaKNwb7IR+Ksu5sLN9djJ1C7sFqWgCPAGOCFMkLUV77S+xH/gr2w3kEa1SKxUvBNm8JfhyfwE53N2I1pD8BXYNlZ2LpqY3Aa9gOHu29AIcDQH3rmw6sEOuJ8hvgOlWtwA6Ms4P3sgr7ET7JsR7F0gMLgvVX0LjgFmk+lrbYEvZcgCV1LP8G1vtph4gcU5uMg08Dp2ENw9/HUhCx+gG2H23E9osnYn2hqu4H7sD2qeKgHHMase3bsVTPDqxh+8/1LFvfd/0b4FNiPZAeCNqpLgXuwn4f3wAuDauVg73Px4JtZwbvoy7zsXa0BXU8h0buY6q6Aqvc/RWLB8UcndKta59vdcTS0a45xLoEFgA3qOqbiS5Pc4nIF7GddmqDCzvXwkRkHtZb50+JLkt74DX9JhKRC0WkW5D6+RZWc2y1Lfb1EZF+IjJFrO/yKKzG9fdEl8s5F39+RW7TnY6d6mUAK4Ergq5dbVEG8EdgCJaPnIV1+XTOtTOe3nHOuSTi6R3nnEsirS6906tXLx08eHCii+Gcc23K4sWLd6tqTkPLtbqgP3jwYPLz8xNdDOeca1NEJKarsz2945xzSaTBoC8ij4oNPbq8jvkiIg+IDUH8oYicHDbvJhH5KPhL2F2RnHPOmVhq+o9hV5vV5SJgRPB3K/B7ALHxrr+PXXk4Gfi+iHRvTmGdc841T4NBPxj6dW89i1wOPB4M1foO0E1E+mHjyL8aDAFbDLxK/QcP55xzLSweOf0BHD0EaUEwra7pxxCRW8XuOpNfVFQUhyI555yLJh5BP9qwrFrP9GMnqj6kqnmqmpeT02CPI+ecc00Uj6BfwNFjXQ/Exqqua7pzzrkEiUfQnwN8JujF8zGgRO12gC8DFwTjfnfHxu1+OQ7ba9eWF5bw+NubqKhqlUNxO+fauAYvzhKRp7G7DvUSkQKsR046gKr+ARuL/mLsJsNlwGeDeXtF5IfAomBVM1W1vgbhpKWqzF9bxMMLN/CfdXYHttmLC/j9jFMY0C3W4fNdU6gqO0orKCguZ2JuN9JT/dIV13yqypKt++jZKYNBPTpiN9drHVrdgGt5eXmaLFfkVlbXMmfpNh5esIE1O/fTJ7sDn5syhH7dsvj288tISxV+e/3JnDki2q1AXWOpKlv3lrN8WwnLC0tYvq2UFYUl7DlYCcCI3p2597KxTBnun3dFVQ1F+w9RUl5FaXmV/a+w/10y0zl9aE9O6Nm6gllrsbywhHvnrCB/czEAXTLTGNs/m3H9uzJuQFfGDchmaK/OpKTE97MTkcWqmtfgcu0l6JdVVnPH0x/QNSud7Kw0umal2+NM+9+1Y9jjrHQy01Ma3GHLK2soKC5ja3EZW/aUsbW4nF37D5HTuQO5PbIY1KMjuT06MrB7Fh0zYh/RYkdJBbMXb+XJd7awo7SCUX26cMvZQ7nspP5kpFlNc0PRAf7rycWs23WAuy4YxRenDovLTlK0/xDPv1/AssISugSfR/jn1aNTBif2zaZHp4bvFFhWWc3qHfvZVVpBaXk1JUFwCAWI7h0zmD6uL6cO7kFqnHfwhtTUKht3H2RFKMAXlrJ8Wwn7K+xueGkpwsg+XRg3IJtxA7rSKSON37z+EVv2ljF9bF++fclocnt0bLHyFe0/xPJtJawoLGHtzgNkpKWE7Z9pZGel061jOiP7dGFg98aXo7K69nCQLi2vorK6luG9O9Ozc4c6X1NaUcUbq3bxz+Xbmb+2iIqq+u8ZP6BbFqcP68mU4T05Y1gv+mRn1rt8Ta2yv6Lq8L5SWlFFTW30+NMnO5NhOZ1Ia0NnXnsPVnL/K2t4+r0t9OiYwVfOG0F6akpQyShl1fZSDlXbZ9onuwPTx/Zl+rh+TB4Sn99H0gX93QcOceMj7x2ulRw4VP/tVNNTJQj+qUSL/eWVtew+cPQdAzPTU+jdJZOi/Ycoj8i59+rcgYm5XTljWC/OGN6TUX26HHVQqaqp5c3Vu3hm0VbeXLOLWoUpw3tyy1lDmToyJ+oBqKyymrufW8acpds4b3QffnHNSXTNSmd/RRUrt5Uerqmu3F5K366ZTAm2Pbpv9lEHiJpaZcHaIp5ZtJXXVu2kulbJ7ZFFeWWtBYSaY3/c/btmMnZA16B2ks3IPl0o3FfO8sISVmwrZXlhCeuLDhD5m00RyA4OtjtLKzhUXUuvzhmcP6YvF4/vy8eG9ow5hVJaUcWKwlI+2rWfQT06MnlIj3oPrnsPVvLqyh38c/kO3tu4l7JK+44y0lIY3S+bcf2zGdu/K+MHdGVk3850SEs96vUVVTU88u+N/O6NddSqctvUYXxx6jCyMlKP2VZFVQ0iHLOOcKpK0f5DbNlrFYeNRQfts9tWws7SI/vWwO5Z1NYqJeVVHKw8ti3nhJ4dbb8a1pMzhvU8HLjtwHbADmjB97Jx90FKyquO2T9DQt9rqOY5NKcT+ZuL+dfyHfz7o91U1tQeDkhjB3Q9uvLU0R7vKq3gP+v38Na63by9YQ/7yqoA6Nc1k7TUY/fj2lr7LkMH3Fh1SEvhxOB7Gxfsi9G+t0SrrqnlqXe38ItX1nCwsoabzxjMHR8fQdes9GOWW190kKUF+3hj1S7mrd1FRdWR38dF4/py+rDYfx+Rki7oR6quqWV/RfXh2s7hGmhYjbS0ooqKKD8ysEAxsHsWuUFtPrd7R3p1zkBEUFX2HKy0H3Pwt2lPGYs27WXznjIAenXO4PRhvTh9aE+2Fpcxe3EBRfsP0btLB67OG8g1ebmc0LNTg+9DVXnsrU38+KVV9O7SgQ7pqWzcffDw/D7ZHTixbzZbi8vYUGTTu3dM5/RhPTl9WC+KSiv42+ICtpdU0LNTBledYtse3rvz4fVXVB2pFe4qPcSq7aWHUyAbdh8kchfpm53JuAEWQMf2z2Zg946HzxY6ZaQdPuAcPFTNvDVFzF2+nTdX76KssoauWelMGd6Tnp06HAkowWvTUlJYu2s/K4JaeeizDElPFSbmduOMYb2YMrwXE3O7sa+8kpdX7OSfy7bz7sa91AQHtHNH9Wb8wG6MG5DNsJzOjfohbS8p56dzVzNn6Tb6d81kVN8uYfuQ7VOVQY2tQ1pK8B5CwTENBbbuLaOguPxwzQ7sgDi8d2fG9e8aHFCzGdM/my6ZR4JDVbDflpRXsfdgJUu37uOt9bt5d8Ne9gcVmRP7dqFThzRWbis9HNxDB7bhOZ3p3vHYs9vUFGHtzv2H01rriw4c9b0O6JbFxeOt5jkpt1vMZ5W1tcrK7aW8tX43q3fsj9opW0Tokpl2+DMK/XXJTCM9ykFCFQr3lbOsoCQ4Gyo9/N67ZKZxxcQBXHtqLuMGdD3mtSGhs5a31u+mW8eM4Ddsv+cB3bLITK//wLG/oir4fZfb2f7essNpwEird+xn3a4DTBnek3s/MZYRfbrUu+6Qssrg97HMfh8HK2sY0y+buXeeFdPrIyV90E+UguIy3lq/h7fX7+E/63aza/8hUlOEc0b15rpTc5k2KqdJp6yLNu3l5/9aTc9OHSzgBrW13l2OnFJvLykPtruHt9bvZntJBSJw9ogcrjs1l4+P7nM4fRSrg4eqWbW9lLU7D9C/WyZj+3clp0vdKYK6VFTVMH9tEf9avoP3txQfTjtEO7sf1KPjUQeVkX26sG7XAd5ab+9rWWEJqnbmdai6FlUYmtOJi8b15aJx/RjbPzsuueZ3N+zhV6+t5eChmqMOTqEzGeConHfoD6z2PiiswhBKAzYUbOpSXVPLssKSw59BZXUtY8NyxI09sB08VM3qHaV8tPNAsJ74fGYtobZW2VpcxrLCEl5buZO5y3dQWV3LuAHZXHvqIC6f2J/szHT2lVXyysqdR521dOuYTlllzeGDNIAI9OmSSccoZ3AKFJdVHj57CenSIc32+ygfUZcOaXxx2nAuHNunyZ9hRVUNC9YWUV5Vw+UTo17D2iAP+q2AquWVO2emHRWcj9e2t+wto0NaKn27Ht9tx6q2VjlYWX34DKy8qobhOZ3p2jG93teVlFXxzsY9vLNhD12z0rl4fD9G9O7caoOWi6+Ssipe+KCQWYu2smp7KZnpKYzpl83SghJqavWYsxaAXfsPsTWosW+JchYWrktmmh2wu3cMDtxZdM1Kb/X7lwd951y7pqosLyxl1qItLCss4czhvbhoXL9WfdbSkmIN+q3uJirOORcLEWH8wK6MHzg+0UVpU9pOfyjnnHPN5kHfOeeSiAd955xLIh70nXMuiXjQd865JOJB3znnkogHfeecSyIe9J1zLol40HfOuSTiQd8555KIB33nnEsiHvSdcy6JeNB3zrkk4kHfOeeSiAd955xLIjEFfRGZLiJrRGSdiNwdZf4JIvK6iHwoIvNEZGDYvBoR+SD4mxPPwjvnnGucBm+iIiKpwIPA+UABsEhE5qjqyrDF7gceV9W/iMi5wE+BG4N55ao6Mc7lds451wSx1PQnA+tUdYOqVgKzgMsjlhkDvB48fjPKfOecc61ALEF/ALA17HlBMC3cUuCq4PGVQBcR6Rk8zxSRfBF5R0SuiLYBEbk1WCa/qKioEcV3zjnXGLEE/Wh3GI68m/rXgakisgSYChQC1cG8QcHNej8N/FpEhh2zMtWHVDVPVfNycnJiL71zzrlGieXG6AVAbtjzgcC28AVUdRvwSQAR6QxcpaolYfNQ1Q0iMg+YBKxvdsmdc841Wiw1/UXACBEZIiIZwHXAUb1wRKSXiITWdQ/waDC9u4h0CC0DTAHCG4Cdc84dRw0GfVWtBm4HXgZWAc+q6goRmSkilwWLTQPWiMhaoA/w42D6aCBfRJZiDbz3RfT6cc45dxyJamR6PrHy8vI0Pz8/0cVwzrk2RUQWB+2n9fIrcp1zLol40HfOuSTiQd8555KIB33nnEsiHvSdcy6JeNB3zrkk4kHfOeeSiAd955xLIh70nXMuiXjQd865JOJB3znnkogHfeecSyIe9J1zLol40HfOuSTiQd8555KIB33nnEsiHvSdcy6JeNB3zrkk4kHfOeeSiAd955xLIh70nXMuiXjQd865JBJT0BeR6SKyRkTWicjdUeafICKvi8iHIjJPRAaGzbtJRD4K/m6KZ+Gdc841ToNBX0RSgQeBi4AxwPUiMiZisfuBx1V1AjAT+Gnw2h7A94HTgMnA90Wke/yK75xzrjFiqelPBtap6gZVrQRmAZdHLDMGeD14/GbY/AuBV1V1r6oWA68C05tfbOecc00RS9AfAGwNe14QTAu3FLgqeHwl0EVEesb4WkTkVhHJF5H8oqKiWMvunHOukWIJ+hJlmkY8/zowVUSWAFOBQqA6xteiqg+pap6q5uXk5MRQJOecc02RFsMyBUBu2POBwLbwBVR1G/BJABHpDFylqiUiUgBMi3jtvGaU1znnXDPEUtNfBIwQkSEikgFcB8wJX0BEeolIaF33AI8Gj18GLhCR7kED7gXBNOeccwnQYNBX1WrgdixYrwKeVdUVIjJTRC4LFpsGrBGRtUAf4MfBa/cCP8QOHIuAmcE055xzCSCqx6TYEyovL0/z8/MTXQznnGtTRGSxquY1tJxfkeucc0nEg75zziURD/rOOZdEPOg751wS8aDvnHNJxIO+c84lEQ/6zjmXRDzoO+dcEvGg75xzScSDvnPOJREP+s45l0Q86DvnXBLxoO+cc0nEg75zziURD/rOOZdEPOg751wS8aDvnHNJxIO+c84lEQ/6zjmXRDzoO+dcEvGg75xzScSDvnPOJZGYgr6ITBeRNSKyTkTujjJ/kIi8KSJLRORDEbk4mD5YRMpF5IPg7w/xfgPOOedil9bQAiKSCjwInA8UAItEZI6qrgxb7DvAs6r6exEZA8wFBgfz1qvqxPgW2znnXFPEUtOfDKxT1Q2qWgnMAi6PWEaB7OBxV2Bb/IronHMuXmIJ+gOArWHPC4Jp4e4FZohIAVbL/3LYvCFB2me+iJwVbQMicquI5ItIflFRUeyld8451yixBH2JMk0jnl8PPKaqA4GLgSdEJAXYDgxS1UnA14C/ikh2xGtR1YdUNU9V83Jychr3DpxzzsUslqBfAOSGPR/IsembzwPPAqjq20Am0EtVD6nqnmD6YmA9MLK5hXbOOdc0sQT9RcAIERkiIhnAdcCciGW2AB8HEJHRWNAvEpGcoCEYERkKjAA2xKvwzjnnGqfB3juqWi0itwMvA6nAo6q6QkRmAvmqOge4C3hYRL6KpX5uVlUVkbOBmSJSDdQA/6Wqe1vs3TjnnKuXqEam5xMrLy9P8/PzE10M55xrU0RksarmNbScX5HrnHNJxIO+c84lEQ/6zjmXRDzoO+dcEvGg75xzScSDvnPOJREP+s45l0Q86DvnXBLxoO+cc0nEg75zziURD/rOOZdEPOg751wS8aDvnHNJxIO+c84lEQ/6zjmXRDzoO+dcEvGg75xzScSDvnPOJREP+s45l0Q86DvnXBLxoO+cc0nEg75zziWRmIK+iEwXkTUisk5E7o4yf5CIvCkiS0TkQxG5OGzePcHr1ojIhfEsvHPOucZJa2gBEUkFHgTOBwqARSIyR1VXhi32HeBZVf29iIwB5gKDg8fXAWOB/sBrIjJSVWvi/Uacc841LJaa/mRgnapuUNVKYBZwecQyCmQHj7sC24LHlwOzVPWQqm4E1gXrc845lwCxBP0BwNaw5wXBtHD3AjNEpACr5X+5Ea9FRG4VkXwRyS8qKoqx6M455xorlqAvUaZpxPPrgcdUdSBwMfCEiKTE+FpU9SFVzVPVvJycnBiK5JxzrikazOljtfPcsOcDOZK+Cfk8MB1AVd8WkUygV4yvdc45d5zEUtNfBIwQkSEikoE1zM6JWGYL8HEAERkNZAJFwXLXiUgHERkCjADei1fhnXPONU6DNX1VrRaR24GXgVTgUVVdISIzgXxVnQPcBTwsIl/F0jc3q6oCK0TkWWAlUA18yXvuOOdc4ojF5tYjLy9P8/PzE10M55xrU0RksarmNbScX5HrnHNJxIO+c84lEQ/6zjmXRDzoO+dcEvGg75xzScSDvnPOJREP+s45l0Q86DvnXBLxoO9ca/XMjbBsdqJL4doZD/quZZTthf07E12KtqukEFbN8aDf2tXWwq7ViS5Fo3jQdy3jxTvhr1cnuhRtV+Fi+7/9g8SWw9Xvw2fgfz8Ge9YnuiQx86DvWsauVbD9Qyjfl+iStE2FwfhT+7dD6fbElsXVbe2/AIVtSxJdkph50HfxV1sL+zYDeiR4ucYpfB/Ssuyx1/Zbp9oa2DDPHu9YltCiNIYHfRd/B3ZATaU93uq3T2i02hqrOY67CiSlTdUik8q2D6AiOJPduTyxZWkED/ou/oo32X9Jga3vJrQobVLRaqg8AEPOgpwTPei3VutfBwRGXAA7POi7ZBYK+kOnQUG+1Vxd7EKNuAPyoN9Eq1G2svteOGD9G9B/Igw5285uD+5OdIli4kHfxV/xJqvlj7vKaqy7Via6RG1LQT5kdoUeQ6H/JDi4C0r91tKtSkWppS6HnQt9xtm0NpLX96Dv4q94E2QPgMFn2nPP6zdO4fsw4BRISbGgD57iaW02LQStsaDfd7xN27kisWWKkQd9F3/Fm6D7YOh2AnTu40G/MSoPwq4VltoB6DsOJNWDfmuz7nXI6AwDJ0OnXraft5HGXA/6Lv6KN0H3E0AEBp7qjbmNse0D0Fqr6QOkZ0HvMd5ts7VZ/wYMPgvSMux5n3FtpjHXg76Lr8oyOLDTavoAuadB8UY4sCuhxWozDjfinnJkWv+TrKbvjbmtw94Ntk8PO/fItL7jrNdVdWVKkMOZAAAeEElEQVTiyhUjD/ouvvZttv/dh9j/3NPsv6d4YlOYD90GQeecI9P6T4KyPVCyNXHlckesf9P+hwf9PuOhtgp2r01MmRohpqAvItNFZI2IrBORu6PM/5WIfBD8rRWRfWHzasLmzYln4V0TlRe33KloqLtmqKbf7yRIzfAUT6wKFh/J54d4Y27rsv4N6DoIeg47Mq1v0IOnDeT1Gwz6IpIKPAhcBIwBrheRMeHLqOpXVXWiqk4Efgs8Hza7PDRPVS+LY9ldU73yHXj4XBsJM94ig356pvU1L1gU/221N/t3QGkBDIwI+n3GQUq6B/3WoKYKNi6A4edam1VIzxFWuWkPQR+YDKxT1Q2qWgnMAi6vZ/nrgafjUTjXAqoqYOUcqDkEy/4W//UXb7ZeDR17HpmWO9m6IbaBfGdCRcvnA6R1gN6jrZHXJVbhYjhUenRqByA1za6ebgONubEE/QFAeDKxIJh2DBE5ARgCvBE2OVNE8kXkHRG5oo7X3Rosk19UVBRj0V2TfPSK7bQdusL7T8S/cTDUXTO8FpQ72Q4yOz6M77bam8LFkJJmKbFI/Sd5Y25rsP4Nu/BwyNnHzus7vt3U9CXKtLr2vOuA2aoaft39IFXNAz4N/FpEhkW+SFUfUtU8Vc3LycmJnO3iafls6NQbzv027FwG25fGd/2hoB9u4GT773n9+hXkQ5+x1k0zUv9JNrhXKH3mEmPd63YmltX92Hl9xsHBolZ/86BYgn4BkBv2fCBQ1zXh1xGR2lHVbcH/DcA8YFKjS9la1FTB4r/YXY3aoopSWPMvGHslTLgW0jJhyZPxW79q9KCf3c96pHjQr1ttrdXkI1M7IcerMVfV7oXw3sPw4d+sncGZsr2w7X0Y9vHo8w835rbu4RjSYlhmETBCRIYAhVhg/3TkQiIyCugOvB02rTtQpqqHRKQXMAX4eTwKnhArXoAX74DUDnDqF+Csr9nVeG3F6pcszTL+asjqBqM/AcuehQt+GL122VgHdkF1uV2JGyn3NNj0bwsqEu3kMcnt+cjSbpE9d0J6j7GGwu0fwLhPxnfbxZth43zYMN8aKQ9GXFORcyIMmWopjcFn2r6TjDYusAvnIvP5IYfH4FkOw887fuVqpAaDvqpWi8jtwMtAKvCoqq4QkZlAvqqGumFeD8xSPSrpOBr4o4jUYmcV96lq2x19a9Ucu9x6+Pnw7u9h8WNw+v+D029vGz+EZX+zgBzqHTJphk1b/RKM/1Tz1x/Zcydc7mm2rZIC6JZ77PxkVxDcbKaumn5ahqV+4lnTrzwIf/nEkQbkTr0tsA+dasG9oiQ4EMyH9x+H9/54ZCC9K/8IKanxK0tbsP4N6JBd93fUsYeNOdXKx+CJpaaPqs4F5kZM+17E83ujvO4tYHwzytd6VJbButdg4qfhkl/AlDth3k9gwf/YqfCUO+C0/4KMTokuaXQHiuwuP1PuPFLTHny2pV2WPHEcgn5YXr+xQX/fFvsxtecgU7jYAkqvkXUv038SLHvOUkEpcbiucsvbtt0zv2rpvpwTjz0L6z8JzvwKVB+yZVe8YMG/53CYdswlO+2XqgX9IWdbT5269Bnb6htz/YrcWK1/A6rKLCUCkDMSrn4MblsIgz4Gr8+E30yEd/5gP5DWZuULNirg+LCblaekwMQZdjAo3tz8bYSCfrdBx87rPRbSOzX+ytyt78FvToIXvti+e64U5luArS+Y958Eh0psCIB42Py2DeZ21tetS2h9abe0DnDCGXDRz+Ck62HefVYJShZ71tkV0XWldkL6jLOrcltjDAi0n6C/fyf8/Yu2I7eEVS9ai/0JU46e3m8CfPoZ+PyrkDMK/vVNeOBka/CtqW6ZsjTFstmWF+4z5ujpEz8NCHzw1+Zvo3gTdOlvF2RFSk2DASc3rjG3uhLmfNm6MX74DOQ/2vwytkZV5ZYSqCttENJvov2PV4pn81vWPbRD59hfIwKX/NL2pedusXRde1dTbalcgOF1NOKG9B0HtdU2Dk8r1X6CfocusOLvsPy5+K+7uhLW/hNGXQyp6dGXyZ0MN/8DPvN/0KWvNfg+eKr1gKiqsHVE/tXWxr+s0ezbAlvfsVxspG65MOwc+OCp5pcnWs+dcLmn2Y0mKg/Gtr5//8p+PNc8YQ1j/7rbLvJqb7Z/aIEi8krcSL1HWyeCeAT9qgpL15xwRuNfm9ERrnncerM9e1P7veiuttbiyf+eBm//DkZcWP/+DTYGD7Tqi7TaT9DP6AgjzoPV/4h/MN200Bq1Qqmd+gydBl94Da6fBWlZ8PwX4Md94Ec5x/7972nHp/tn6EBYV95+0gw7dd04r3nb2be54aCvNbEF7qI1sPB+GPcpGDUdPvmwNaI/e1PLDB+RSIUNNOKGpKbbBUDxuDJ32/vWk6spQR+g13C4/HdW9le+0/zytCaqsOaf8MezYfbnrNfUdX+1M/qG9Bxmv/tW3JgbU0NumzH6MkvDFC6G3FMbXn7vRrvgpX8Dlw6setHy0UPPia0cIjDqIqsZrP5H9JH3amvgrd/C45fDZ+dC596xrbsplj1n49rXFZBPvNRSV0uebDhnWZeqCrulX31BP1STLXjPbvpdl9pamHOHNYpPv8+mdewBV/8FHr0Q/n4bXP9MfBozW4PCxZA90M4QG9J/Eix9uvmNuZvfsv+DTm/6OsZeAVu/BO88aGe68egMkGjblsDc/7axonoMhU/+ybrIxtqJICXVzshacV/99hX0R1xgA1OtmhNb0H/+Vrt/6x0fHD2UbbjaGuvSOPKC6Lnq+qSkwJh6xpgbcjY8+Ul4/ApLDXXs0bj1x2LXatsBL6rn8oi0DjD+Gstblu1tWjlKtgJaf9Dv2MN6pzTUmLv4UUtHXfGHo7+XgafA9J/C3K/Dv38BZ/9348vZGhXk23uLRf9JsOhha1jMqaenT0M2vwU5o5u/z53/A6vtz7nDzkJyRjVvfYn2/K1Qvg8+8RuYeEPd6dz69BlrMaOVXpPSTqpKgaxu1sd41YsN9/TYtdpqnJUHYP59dS+39T27WCWW1E5jnXC6nTbuWQdPXGkppHhbPtv6Vo+9sv7lJs0IBmGb3bTt1NddM1zuZGts3/Sf6PNLCuHVe+2s6qTrjp1/6hesB9KbP7FeR4my4H7rsdXcfPae9ZYWayi1E9I/aMxtzp20aqptvz6hGbX8kNR068WWnmWpt5qq5q8zUUq321n5lDvglJubFvDBDn7le2H/9rgWL17aV9AHC87FGxvOqX3wpPUKGXMF5P8Zdn8UfblVL1pOb8QF8S8rWCPqtU9YeZ+6Gg4diL5cbW3j8/+qdkHUkLMbTh/1m2A9OZY80bhthMQa9PM+b+0vj11sB7rQhUGh8r50l+X9P/Hr6LUkEbj01zaU7ezPW0opHhrTTlBSYAedhb+wi5tKm/jjLt1mn0FmV0tNxqLXKMsZN6cxd+cyqNx/bE+0psruD5c9AEWrWmbk1uNl00L7P7ie1GMswq/MbYXaX9AfdQkgFqzrUlMFS2fByOlw8f9YLeW1e49dTtXWM+xc6x3UUkZeCJ96xPKIs663LnyqVgvMfxT+djPcPxx+NQaWP9/g6g4rfN+CcXjf/PpMutFGwgzdGagxijdZMGro4DLgZLhjCVzwI2uQfPhcmHUD7Fxp1xKs/Sec8+36Dx4dOtuBsqrc8vvN6b9fXWk53J8PgdVzG14egq6jChf82Hoj/fHsus9c6nJgF/zlMjvYzPg79BgS2+tS0+wA3ZygH+rW3Jx8fqRRF1sNd+EvLSXaFm2cD5nd7H00R5+x9r+V5vXbX9DvnGM9EuoL+mtfttHwJt1oQWrKndbguuWdo5fbvhRKtrRMaifSmMsth71xIfzpPPj1ePjtyfCPr8KWd+1Mo9sgePvB2Ne5fLadpZx4aWzLT5oBPYbZNivLGlf+8JuhNyQ9C874Mty5FKZ9y8Y0+f0Z8MKXLGd92n81vI6cUXDhj+y1HzzVuLKGlG6Dxy6B9x6CjC4w76cNH0CqKqztY+RFcMbtcMvrkJltNf63fhfbAahsr7XjlBbCDX+LPZ8f0neCHWya2ktt839sOI6uUUdIbxoROOsuG0NoVRu9Qd7GhTb8RHOv/M7qZnfWaqU9eNpf0AcL0rtWWE05miVPQue+RwZFOv1L9vyV7x79o131ol2xOPKili8zwEnXWje4qjILfhffD7fnw9dWwpV/sDF+CvPtlnoNqSi1YDjqotjHBUrPstP04o31t3NE01Af/Wgys2HaNy34T7nTeq9c9tv6L3MPd/LNMOgMePnbjR/OduNCq6HvXAGf+jNcdJ+d5TR0leny5+x+tafdZs97j4Zb3rTP+ZVv21nZof11v76ixFI6e9ZZe05T8ur9Jlhb1N4NjX+tqlVumtpVsz6jL7O024JftL2rp4s3WdtKtHHym6LvOE/vHFcnXmL/o9X29++wG4lMvP5IcMnoBOd8yxp2w2spq16EwVOgU89j19NSJs2w9Me1T8DkW6DXiCO155Out7tSvffHhtez6E8WYM78auO2P/hMOPkzVmuNtT94XUMqx6pjD+sFcsf7jTu1Tkmxg1RVmV0JHQvVI11lM7vBLW9Yl7wJ10LXXGugrStgqcK7f7BeL+HBITMbrn0Szp9p+89v8+CF/wdLnzk633/ogLXb7Fxh3++wGLsARwrdZGVHE+6FsPsjKNvdMkE/JdX2t53L7DfWlmyMUz4/pM9YO+upKo/P+uKofQb9boPskvVoQX/p09ZQOHHG0dMn3mA/5tfutTxv0VrYvSb2BrbjITPbyrn8ecsJ16WyzNJAwz7e8DUI0Zw/04aMnvPl2IaSKNtjNc+mBv3m6DUCzv6GXY295p/1L3toP/ztJruY6MSLLeD3PtHmpabb2cbWdyz9Ec3Wd+1sYPItx6axROz1N71oPZTWzIW/3wq/PBF+dyq89HX46zXWbvOpR6wdp6lyRlvX5O1NuBNZ6L0NaoGgDzDhGktt1HfwbI02LYSOvezMLR76jLNhmHetis/64qh9Bn2wFE9h/tE9XlQttTPoDLuiMFxqmtU2926wnO3q4IAROmtoLSbfCrVVR8YCieb9x602d9ZdTdtGVndr4N7xoV1405BYe+60lCl32lgwL91laa1oitZYo/GqF+2gds0TdhANN2mGDS+84P7o63j3j3abyQnX1l2WwWdaLf6/N8BtC+D8H1r+/IO/2qiWV/zB2m+aIy3DDlZNuf3klrftPfY85gZ28ZGabl0eC9470humtVO1tqEhZ8evX/2AU6yr9D++Ep/BDOOoHQf9oIa++qUj07a+a7nUSTOiv2bEBXZ6N/8+GzNn4KnWHa016TXcavCLHoneJ7q6Et56wHpmDG5Gl7zRl1kD8Js/qbttJCTRQT8tw9oCSrdZ3/lIK16wgF+218ZGCh9eOlx6ljXObnjz6K6kYGmaVXPg5BtjG6AsJcXSMFPugBmz4Zub4BsbrN0mHvqeZDX9xtamN79l7QgtedHQpBttyIy6Dp5gQ33/42utYyylPeutT319V4k3Vrdca7PZuwkemtqqRiRtv0E/Z6T1aQ7P0b//hOXE66ppidhdpMr2WJ/jWHu9HG+n3QYHdsDK/zt23tKnrVfIWV9v3jZErLafmmG1lfqCy+EhlaPcMet4GZhnvX4W/elIL6yaamvk/dtNdtp+24KGG+ryPme5/gW/OHp6/qPWFfHULzStfGkZ0e+r2lT9JtjZXGMuANq3xa6cjlf//LqkZ1qng43zj9wcJtzWRdaInv8IPPeF+OS9mzOi7cb59n/I1OaXI9yoi+DWN23k2Sc/BfP/5/gNsliP9hv0wVI8m/8DB/dYPnfF363Rrr6aWv9JR/q1H4+umk0x/HzoPsS6GoarqbaRKftNbHgI2Fhk97eUV0PdIos3Wc0uo2Pzt9kc534Hug60IQH2bbXG2rd/B6feAjfPja2LYocudvBY89KRLnfVh2Dxny0PH2t/+pYWasxtTF6/Jfrn1yXvc3aQC6/tq9pB+c8XWTp1+s9g73qY34w7qFaWwfO3wc+H2lhaTbFpoQXmHkObXo669BxmAzCOvxre/JFdh1O+L/7baYT2H/S11hrVVrwAVQft1LMhF98PM55vubxnc6WkWG5/67tHX6Sz8gXrbnnWXfE7fY+lW2Rzeu7EU4fOcOmvrAH+tydbiubKP8Il91tNO1an3WZnhAt/ac9X/N2u65h8a8uUuyn6jAPEriWJ1Za3rE0idPFQS+rQGU77ol1st2O5BecXvmjtLkOnwq3z4WP/ZanW//zGrjtorD3r4ZHz7V4L1eXwxg8bvw5V67kTz3x+pIyO8MmHLK6sew0emmbxKEGjxbbvoN/vJOtJsOpFG16g1yjL0zckq1t8asotadINNvLnu0Ftv7bWhgXIOTG+aalQt8jqCnj+luhXWxY3MKTy8TTifBs3pftg+MKr0cfvaUjHHlZTXfG8BZZ3/2ADxTV1BNKW0KGzVUoa05i7+S0YdNrxu+3kabfaRW+vfhceucCugp96N3z6b0cGejv/h/Y41p5iIWv+CQ+dY6nMGbOtnWb5c8e2xTRk1ypLk8Uznx+NiPX6unku1FRayvHnQy3N9cp34aPXYr/PRDO176AvYrX99a9brXjSjFY56l2TZHa1gLb8OTi422pUu1bCmV+L/5DDvUZYLWXjfLtqNVx1JZQWJDafH+nSX8OX3mve5fSn327dIp+/xc6mJt/a+vadvhNiT+8cKLLBxFqif35dsrrDqZ+3W42WbIFPPwvn3HP0/tmxh40Au22JHVwbUlsDb/wInr4Oegy2M4bh51nQ79gLXvle4xq3Ny6w//G6KKshg06zixE/9zJMu8cOiu/+AZ66Cu47AZ6JIRPRTO076IMF/dpqG1ytKbW+1mzyrTYy5uLHLHfafXD0u2PFw8k32kFzwf/A2rALb0q2WgqttdT0wYJzcwN0lz52kVrhYvthtsZ9p98EC6axpAm2hPL5xzHog91UfcqdFpxH1jFo4dgr7ar3N35Uf17+4B546lO2D06aAZ97xYb+AGuLmXY3bP43rP1X7OXbtNAqLNHu69xSUtPtvtrTvgmffQm+udnSyad/yc7UW1j7D/q5kyF7gA0I1ZI3KkmE3ifanboW/sLuhDTlK7EPYdAUF99vt4N7/pYjfY8T3V2zJU2503ovnfyZlh1wr6n6TrD/seTDt7wNaZlNu1ivObK623UR9TWAi8Alv7CK2T++emxN/dB+a+x9YCJs+jd84gG4/MFj729xys3Qczi8+r3YUkW1NRb0j1ctvy4ZHS2dfP4P4Nxvt/jmYgr6IjJdRNaIyDoRuTvK/F+JyAfB31oR2Rc27yYR+Sj4uymehY9JSqq1nl/eiIHK2pLJt9kwBF36Bzc5b0HpWXDNX6xm/7ebrFdLew763XLhS+/Cx7+X6JJEd3g4hhhSPJvfsvasxjRoH09dB8B537drJJbOsmlV5TYcyG9Ogjd/bNfQ3LYATqkjjKSmw3n3WhorliHCdyyzoUri3VWzlWuwWigiqcCDwPlAAbBIROao6srQMqr61bDlvwxMCh73AL4P5AEKLA5eWxzXd9GQ1naBVTyNvNCGiB53ld0Bq6X1HAZX/B6euQH+dY+NW5SaAV36tfy2E6EluvHFS6dedhbbUA+eilI7MLT2O43lfd7G43/5HigvtosM92+3G+qc+93YRiM98VLI/ZhdVDj+6vq7Zx/O57dwI24rE0tNfzKwTlU3qGolMAuo7zry64Gng8cXAq+q6t4g0L8KTG9OgV2ElFS7YfOEa47fNkdfCmfcYRfXfPiM5UTby/1q25pYGnO3vG1nZ8ejf35zpKRY6qbyoAX+bifAzS/BZ16Iffjp0AWWB3fZNRr12bjAemXFcm/idiSWX+oAYGvY84Jg2jFE5ARgCPBGY14rIreKSL6I5BcVFcVSbpdoH/++NQoe2Nk+UzttRb8JNppjffc/WPq05daPZ8+dpup9Ilw/yxo2P/cvG8uosXIn21X3/3mg7mtLaqrsYBivUTXbkFiCfrRuEHX1iboOmK2qoc7cMb1WVR9S1TxVzcvJqeMG5a51SU2Dq/9sbQmh3LI7/vpOsFp8XTfsKNtr409NuPb4pP/iYfjH7a85PbA+/n3r2RbZxThk2xIbGTbRjbgJEEtXjwIgN+z5QKCuG5NeB3wp4rXTIl47L/biuVatS18b+7+tBJP2qF+oB89SyI1y4eGHz9rFQHUNMthe9RxmbQSLHrZ2pyFT7UwnlOMP5fOTsKYfS9BfBIwQkSFAIRbYj+kmIiKjgO7A22GTXwZ+IiKhkaYuAO5pVold6xLZbc4dX11zLXUTLa+var1Y+k1s/n1f26Jpd9tQ6e89bPn9lDQb8njIVLvJS59xx/cGSa1Eg0FfVatF5HYsgKcCj6rqChGZCeSramgYy+uBWapHOtmq6l4R+SF24ACYqaqJGXDCufZIJLhnbpSgv30p7Fxu11cko449bIiGqnK7In/DfLuqfOH9lhI7/fZElzAhYrqSR1XnAnMjpn0v4vm9dbz2UeDRJpbPOdeQfhPsBi81VdZXPWTJE3ZBVmjU2GSVnmUXMQ6dZs8rSuxK6wGNvCF9O+H97Jxr6/qeZHn7ojVHplWVW5/30Z+wAQTdEZldbfC8zK6JLklCeNB3rq073JgbluJZ/ZLVaJOtAdc1yIO+c21dz+GQ3vHoxtz3H7dBxAYnX5dEVz8P+s61dSmp1hMlVNMv3mwNlhNn+JXS7hi+RzjXHvSbYAOI1dYGt7aUlh+Az7VJHvSdaw/6ToBDpdYvfclTMOwcGyXUuQge9J1rD0KNuW89YHcy8wZcVwcP+s61B73H2BWn7z8Omd1g1CWJLpFrpTzoO9cepHUIbrWnNriaD4/h6uBB37n2IjTaqad2XD1a8IaqzrnjavItdqevUH7fuSg86DvXXvSfdPxvfO7aHE/vOOdcEvGg75xzScSDvnPOJREP+s45l0Q86DvnXBLxoO+cc0nEg75zziURD/rOOZdERFUTXYajiEgRsLkZq+gF7I5TcdoSf9/Jxd93conlfZ+gqjkNrajVBf3mEpF8Vc1LdDmON3/fycXfd3KJ5/v29I5zziURD/rOOZdE2mPQfyjRBUgQf9/Jxd93conb+253OX3nnHN1a481feecc3XwoO+cc0mk3QR9EZkuImtEZJ2I3J3o8rQkEXlURHaJyPKwaT1E5FUR+Sj43z2RZYw3EckVkTdFZJWIrBCRO4Pp7f19Z4rIeyKyNHjfPwimDxGRd4P3/YyIZCS6rC1BRFJFZImI/CN4nizve5OILBORD0QkP5gWl329XQR9EUkFHgQuAsYA14vImMSWqkU9BkyPmHY38LqqjgBeD563J9XAXao6GvgY8KXgO27v7/sQcK6qngRMBKaLyMeAnwG/Ct53MfD5BJaxJd0JrAp7nizvG+AcVZ0Y1j8/Lvt6uwj6wGRgnapuUNVKYBZweYLL1GJUdQGwN2Ly5cBfgsd/Aa44roVqYaq6XVXfDx7vxwLBANr/+1ZVPRA8TQ/+FDgXmB1Mb3fvG0BEBgKXAH8KngtJ8L7rEZd9vb0E/QHA1rDnBcG0ZNJHVbeDBUigd4LL02JEZDAwCXiXJHjfQYrjA2AX8CqwHtinqtXBIu11f/818A2gNnjek+R432AH9ldEZLGI3BpMi8u+3l5ujC5Rpnlf1HZIRDoDzwFfUdVSq/y1b6paA0wUkW7A34HR0RY7vqVqWSJyKbBLVReLyLTQ5CiLtqv3HWaKqm4Tkd7AqyKyOl4rbi81/QIgN+z5QGBbgsqSKDtFpB9A8H9XgssTdyKSjgX8p1T1+WByu3/fIaq6D5iHtWl0E5FQpa097u9TgMtEZBOWrj0Xq/m39/cNgKpuC/7vwg70k4nTvt5egv4iYETQsp8BXAfMSXCZjrc5wE3B45uA/0tgWeIuyOc+AqxS1V+GzWrv7zsnqOEjIlnAeVh7xpvAp4LF2t37VtV7VHWgqg7Gfs9vqOoNtPP3DSAinUSkS+gxcAGwnDjt6+3milwRuRirCaQCj6rqjxNcpBYjIk8D07DhVncC3wdeAJ4FBgFbgKtVNbKxt80SkTOBhcAyjuR4v4Xl9dvz+56ANdqlYpW0Z1V1pogMxWrAPYAlwAxVPZS4kracIL3zdVW9NBned/Ae/x48TQP+qqo/FpGexGFfbzdB3znnXMPaS3rHOedcDDzoO+dcEvGg75xzScSDvnPOJREP+s45l0Q86DvnXBLxoO+cc0nk/wPFxX6uw6/5ogAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_train: mean =  0.9867447916666666  max =  0.9928385416666666  min =  0.9817708333333334\n",
      "pred_test: mean =  0.7241666666666667  max =  0.7760416666666666  min =  0.6744791666666666\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADWhJREFUeJzt3X+MZXdZx/H3h51WLNB0awcCLcuU0DZWYgBHwo9YSWvj6iLVxEAb0RaJ+4eCYBRdogkJ/rMK/sCUaNZSaLS0MaUKusV2LTRE0jZMf0C7XdpCu9KllR2oRCsmpfD4xz3EzbB1595z7r0z332/ksk999xz7/d55t75zHfOufdMqgpJUnueMe8CJEnTYcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGrUwy8FOO+20WlpamuWQkrTp3XHHHV+vqsVx7zfTgF9aWmJlZWWWQ0rSppfk3ya5n7toJKlRBrwkNcqAl6RGGfCS1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWpUTP9JKu0US3t2ju3sQ/u3jG3sdU2Z/CS1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGmXAS1KjjhnwSa5McjjJvUese1+SLyb5QpK/T3LKdMuUJI1rPTP4jwDb16zbB7y0qn4UeAB498B1SZJ6OmbAV9VngMfXrLupqp7qrt4GnDGF2iRJPQyxD/5XgU8O8DiSpAH1Cvgkvw88BVz9/2yzM8lKkpXV1dU+w0mSxjBxwCe5FHg98EtVVU+3XVXtqarlqlpeXFycdDhJ0pgm+o9OSbYDvwf8ZFV9a9iSJElDWM/bJK8BbgXOSXIoyVuBy4HnAPuS3J3kr6ZcpyRpTMecwVfVJUdZ/aEp1CJJGpCfZJWkRhnwktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjTLgJalRBrwkNcqAl6RGGfCS1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWpUccM+CRXJjmc5N4j1p2aZF+SB7vLrdMtU5I0rvXM4D8CbF+zbhdwc1WdBdzcXZckbSDHDPiq+gzw+JrVFwFXdctXAT8/cF2SpJ4m3Qf/vKp6DKC7fO5wJUmShrAw7QGS7AR2Amzbtm3aw2kAS7v2zm3sg7t3zG1sqTWTzuC/luT5AN3l4afbsKr2VNVyVS0vLi5OOJwkaVyTBvwngEu75UuBjw9TjiRpKOt5m+Q1wK3AOUkOJXkrsBu4MMmDwIXddUnSBnLMffBVdcnT3HTBwLVIkgbkJ1klqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjTLgJalRBrwkNcqAl6RGGfCS1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJalSvgE/yW0n2J7k3yTVJnjlUYZKkfiYO+CSnA78JLFfVS4EtwMVDFSZJ6qfvLpoF4AeTLAAnAY/2L0mSNISFSe9YVV9N8n7gK8D/ADdV1U1rt0uyE9gJsG3btkmH03FiadfeeZcwc/Pq+eDuHXMZV7PTZxfNVuAi4EzgBcCzkrx57XZVtaeqlqtqeXFxcfJKJUlj6bOL5qeAh6tqtaq+DVwPvGaYsiRJffUJ+K8Ar0pyUpIAFwAHhilLktTXxAFfVbcD1wF3Avd0j7VnoLokST1NfJAVoKreA7xnoFokSQPyk6yS1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjTLgJalRBrwkNapXwCc5Jcl1Sb6Y5ECSVw9VmCSpn4We9/8A8M9V9YtJTgROGqAmSdIAJg74JCcD5wGXAVTVk8CTw5QlSeqrzy6aFwOrwIeT3JXkiiTPWrtRkp1JVpKsrK6u9hhOkjSOPgG/ALwC+Muqejnw38CutRtV1Z6qWq6q5cXFxR7DSZLG0SfgDwGHqur27vp1jAJfkrQBTBzwVfXvwCNJzulWXQDcN0hVkqTe+r6L5u3A1d07aB4C3tK/JEnSEHoFfFXdDSwPVIskaUB+klWSGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSo/qei0ZTtLRr77xLkLSJOYOXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjTLgJalRBrwkNcqAl6RGGfCS1CgDXpIa1Tvgk2xJcleSfxqiIEnSMIaYwb8DODDA40iSBtQr4JOcAewArhimHEnSUPrO4P8c+F3guwPUIkka0MQBn+T1wOGquuMY2+1MspJkZXV1ddLhJElj6jODfy3whiQHgWuB85P87dqNqmpPVS1X1fLi4mKP4SRJ45g44Kvq3VV1RlUtARcDn6qqNw9WmSSpF98HL0mNGuR/slbVLcAtQzyWJGkYzuAlqVEGvCQ1yoCXpEYZ8JLUKANekhplwEtSowx4SWqUAS9JjTLgJalRBrwkNcqAl6RGDXIuGknaDJZ27Z3b2Ad375j5mM7gJalRBrwkNcqAl6RGGfCS1CgDXpIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRk0c8ElemOTTSQ4k2Z/kHUMWJknqp8/Jxp4Cfruq7kzyHOCOJPuq6r6BapMk9TDxDL6qHquqO7vl/wIOAKcPVZgkqZ9B9sEnWQJeDtw+xONJkvrrHfBJng18DHhnVf3nUW7fmWQlycrq6mrf4SRJ69Qr4JOcwCjcr66q64+2TVXtqarlqlpeXFzsM5wkaQx93kUT4EPAgar60+FKkiQNoc8M/rXALwPnJ7m7+/rZgeqSJPU08dskq+pfgQxYiyRpQH6SVZIaZcBLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktQoA16SGmXAS1KjDHhJalSff9k3U0u79s67BKkp8/yZOrh7x9zGPp44g5ekRhnwktQoA16SGmXAS1KjDHhJapQBL0mNMuAlqVEGvCQ1yoCXpEYZ8JLUKANekhrVK+CTbE9yf5IvJdk1VFGSpP4mDvgkW4APAj8DnAtckuTcoQqTJPXTZwb/SuBLVfVQVT0JXAtcNExZkqS++gT86cAjR1w/1K2TJG0Afc4Hn6Osq+/bKNkJ7OyuPpHk/h5j9nUa8PU5jj9r9tu2Tdtv/mjsu2zaXr9nzJ7X9vuiScbsE/CHgBcecf0M4NG1G1XVHmBPj3EGk2SlqpbnXces2G/bjqd+j6deYbh+++yi+RxwVpIzk5wIXAx8om9BkqRhTDyDr6qnkrwNuBHYAlxZVfsHq0yS1Euv/8laVTcANwxUyyxsiF1FM2S/bTue+j2eeoWB+k3V9x0XlSQ1wFMVSFKjmgj49ZwyIckbk9yXZH+Sjx6x/o+7dQeS/EWSo739c0M5Vr9J/izJ3d3XA0m+ecRtlyZ5sPu6dLaVT2bSfpO8LMmt3fP7hSRvmn314+vz/Ha3n5zkq0kun13Vk+v5et6W5Kbu5/e+JEuzrH0SPfsdL6+qalN/MTrA+2XgxcCJwOeBc9dscxZwF7C1u/7c7vI1wGe7x9gC3Aq8bt499e13zfZvZ3QAHOBU4KHucmu3vHXePU2x37OBs7rlFwCPAafMu6dp9XvEug8AHwUun3c/0+4XuAW4sFt+NnDSvHuaVr+T5FULM/j1nDLh14APVtV/AFTV4W59Ac9k9I3+AeAE4GszqXpy454i4hLgmm75p4F9VfV4973YB2yfarX9TdxvVT1QVQ92y48Ch4HFKdfbV5/nlyQ/BjwPuGmqVQ5n4n67c18tVNU+gKp6oqq+Ne2Ce+rz/I6dVy0E/HpOmXA2cHaSzya5Lcl2gKq6Ffg0o5ndY8CNVXVgBjX3se5TRCR5EXAm8Klx77uB9On3yNteyegH48tTqHFIE/eb5BnAnwDvmnKNQ+rz/J4NfDPJ9UnuSvK+7iSIG9nE/U6SVy0E/HpOmbDAaDfN6xj9RrwiySlJXgL8MKNP4Z4OnJ/kvCnWOoR1nSKiczFwXVV9Z4L7bhR9+h09QPJ84G+At1TVdweub2h9+v114IaqeuRptt+I+vS7APwE8DvAjzPa7XHZ0AUObOJ+J8mrFgJ+PadMOAR8vKq+XVUPA/czCvxfAG7r/rR7Avgk8KoZ1NzHuk4R0bmYI/58H/O+G0WffklyMrAX+IOqum0qFQ6rT7+vBt6W5CDwfuBXkuyeRpED6vt6vqvb3fEU8A/AK6ZS5XD69Dt+Xs37oMMABy0WGB0sPJP/O2jxI2u22Q5c1S2fxuhPpB8C3gT8S/cYJwA3Az8375769tttdw5wkO6zDt26U4GHGR1g3dotnzrvnqbY74ndc/rOefcxi37X3H4Zm+Mga5/nd0u3/WJ3/cPAb8y7pyn2O3ZebfoZfI1+c3/vlAkHgL+rqv1J3pvkDd1mNwLfSHIfo31Y76qqbwDXMdonew+jb/Tnq+ofZ97EGNbZL4x2RV1b3Suju+/jwB8yOo/Q54D3dus2rD79Am8EzgMuO+JtZy+bWfET6NnvptPz9fwdRrtnbk5yD6PdH389u+rH1/P5HTuv/CSrJDVq08/gJUlHZ8BLUqMMeElqlAEvSY0y4CWpUQa8JDXKgJekRhnwktSo/wWtlax9PZaSkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split train and test\n",
    "#X_train, X_test, y_train, y_test = train_test_split(Xvec, yvec, test_size=0.20)\n",
    "pred_train = [] \n",
    "pred_test = [] \n",
    "\n",
    "for i in range(50): # entre 10 y 50...aleatorio total.\n",
    " X_train, X_test, y_train, y_test = train_test_split(Xvec, yvec, test_size=0.20)\n",
    "\n",
    "\n",
    "# My Model\n",
    " #clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)\n",
    " clf = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    " prediction_train = clf.predict(X_train)\n",
    " prediction = clf.predict(X_test)\n",
    "\n",
    " pred_train.append(np.mean([prediction_train == y_train]))\n",
    " pred_test.append(np.mean([prediction == y_test])) \n",
    "\n",
    "\n",
    "plt.plot(pred_train)\n",
    "plt.plot(pred_test)\n",
    "plt.title('Logistic Regression with Tdfidf and stop words')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pred_test)\n",
    "print('pred_train: mean = ', np.mean(pred_train), ' max = ', np.max(pred_train),  ' min = ', np.min(pred_train), )\n",
    "print('pred_test: mean = ', np.mean(pred_test), ' max = ', np.max(pred_test), ' min = ', np.min(pred_test), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import manifold\n",
    "n_components = 2\n",
    "perplexity = 10\n",
    "tsne = manifold.TSNE(n_components=n_components, init='random',random_state=0, perplexity=perplexity)\n",
    "X_tsne = tsne.fit_transform(Xvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tsne.shape\n",
    "X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yvec2=yvec.tolist()\n",
    "\n",
    "b,w = np.unique(yvec.tolist(), return_inverse=True)\n",
    "print(b)\n",
    "print(np.unique(w))\n",
    "aux = np.unique(w)\n",
    "\n",
    "#dic = dict(zip(b,aux))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "#With TSNE\n",
    "plt.figure()\n",
    "scatter = plt.scatter(X_tsne[:,0], X_tsne[:,1],c=w,cmap=cm.get_cmap('Accent'))\n",
    "plt.title(\"T-SNE \")\n",
    "plt.xlabel(\"1st Component\")\n",
    "plt.ylabel(\"2n Component\")\n",
    "\n",
    "#a√±adir leyenda (ni puta idea que fa aqui)\n",
    "labels = np.unique(w)\n",
    "labels2= b\n",
    "handles = [plt.Line2D([],[],marker=\"o\", ls=\"\", \n",
    "                      color=scatter.cmap(scatter.norm(yi))) for yi in labels]\n",
    "plt.legend(handles, labels2,loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# this is only for the last iteration of the model performance\n",
    "def plot_confusion_matrix(y, y_pred):\n",
    "    plt.imshow(metrics.confusion_matrix(y, y_pred),\n",
    "               cmap=plt.cm.Blues, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('true party')\n",
    "    plt.xlabel('predicted party')\n",
    "    plt.xticks( np.arange(6), yvec )\n",
    "    plt.yticks( np.arange(6), yvec )\n",
    "    \n",
    "    \n",
    "print (\"classification accuracy:\", metrics.accuracy_score(y_test, prediction))\n",
    "plot_confusion_matrix(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.confusion_matrix(y_test, prediction) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Avui hem repr√©s la Comissi√≥ Mixta amb el @gove...</td>\n",
       "      <td>67</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Torra anunci√≥ un \"oto√±o caliente\" para aumenta...</td>\n",
       "      <td>856</td>\n",
       "      <td>1501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Dem√† cal sortir als carrers per dir que #Barce...</td>\n",
       "      <td>144</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>‚ÄúCerc√†vem or i vam baixar a la mina.\\nI la fos...</td>\n",
       "      <td>338</td>\n",
       "      <td>1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Molt senzill d'entendre, companya: \\n1.- L'ALL...</td>\n",
       "      <td>4932</td>\n",
       "      <td>7253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               text  retweet_count  \\\n",
       "0   0  Avui hem repr√©s la Comissi√≥ Mixta amb el @gove...             67   \n",
       "1   1  Torra anunci√≥ un \"oto√±o caliente\" para aumenta...            856   \n",
       "2   2  Dem√† cal sortir als carrers per dir que #Barce...            144   \n",
       "3   3  ‚ÄúCerc√†vem or i vam baixar a la mina.\\nI la fos...            338   \n",
       "4   4  Molt senzill d'entendre, companya: \\n1.- L'ALL...           4932   \n",
       "\n",
       "   favorite_count  \n",
       "0             103  \n",
       "1            1501  \n",
       "2             174  \n",
       "3            1560  \n",
       "4            7253  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_excel('test.xlsx') # Load the `train` file\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text_noSymbHashtags'] = [filter_symb_hashtag(row['text']) for index, row in test.iterrows()]\n",
    "# to spanish? with and without mentions and hashtags\n",
    "#train_df['text_noEM_sp'] = [translate_text(row['text_noEm']) for index, row in train_df.iterrows()]\n",
    "test['text_noSymbHashtags_more']=[special_char(row['text_noSymbHashtags']) for index, row in test.iterrows()]\n",
    "test['text_noSymbHashtags_more']=[filter_single(row['text_noSymbHashtags']) for index, row in test.iterrows()]\n",
    "test['text_noSymbHashtags_more']=[lower_text(row['text_noSymbHashtags']) for index, row in test.iterrows()]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.50,ngram_range=(1, 1),stop_words=stop_words(),min_df=1)\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_df['text_noSymbHashtags']).toarray()\n",
    "y_train = train_df['party'].values\n",
    "\n",
    "# 1 Train the classifier\n",
    "clf = SGDClassifier(loss='hinge', penalty='l2',alpha=1e-5, max_iter=80)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 2 Predict the data (We need to tokenize the data using the same vectorizer object)\n",
    "X_test = vectorizer.transform(test['text_noSymbHashtags_more']).toarray()\n",
    "prediction = clf.predict(X_test)\n",
    "\n",
    "# 3 Create a the results file\n",
    "output = pd.DataFrame({'Party': prediction})\n",
    "output.index.name = 'Id'\n",
    "output.to_csv('sample_submission.csv')\n",
    "\n",
    "# TIP - Copy and paste this function to generate the output file in your code\n",
    "def save_submission(prediction):\n",
    "    import datetime\n",
    "    t = datetime.datetime.now().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    output = pd.DataFrame({'Party': prediction})\n",
    "    output.index.name = 'Id'\n",
    "    output.to_csv(f'sample_submission{t}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
